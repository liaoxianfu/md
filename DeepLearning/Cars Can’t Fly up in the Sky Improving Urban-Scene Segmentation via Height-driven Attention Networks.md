### Cars Can’t Fly up in the Sky: Improving Urban-Scene Segmentation via Height-driven Attention Networks



#### 摘要

本文利用城市场景图像的内在特征，提出了一种通用的高度驱动注意力网络(HANet)用于改进城市场景图像的语义分割。它根据垂直像素的位置强调信息性的特征或有选择的类别。城市场景图像中水平分段段之间的像素级分布明显不同。同样，城市场景图像也有其鲜明的特点，但是大多数语义分割网络并没有在体系结构中反映出这种独特的属性。我们提出的网络结构包括了这种属性来有效处理城市数据集的能力。我们验证了在采用HANet的情况下各种语义分割模型在两个数据集上mIoU的提高。这种广泛的定量分析证明了在已存在的模型中添加我们模型是简单和划算的。在基于ResNet101的分割模型中，我们的方法在城市景观基准上取得了新的最先进的性能。同时，通过对注意图的可视化和解释，证明了该模型与城市场景中的实际情况是一致的。我们的代码和经过培训的模型是公开提供的。



#### 1、介绍

语义图像分割是计算机视觉中的一项基本任务，用于自动驾驶中对城市场景的基本理解。完全卷积网络(FCNs)[28]是一项开创性的工作，它采用深度卷积神经网络(CNNs)进行语义分割，在典型的CNN结构的最后阶段用卷积层代替全连接层。在其他先进技术中例如在编码-解码结构中采用skip-Connections，atrous卷积，ASPP网络和金字塔池化模型，在语义分割性能方面进一步改进了基于FCN的体系结构。它们已经在不同的语义分割基准[14，15，31，12，1，25，32]中被证明是成功的，包括城市场景数据集。

俄日前往城市景观图有着他们独特的几何远景性质联系和位置模式。由于城市场景图像是由安装在汽车前侧的摄像头拍摄的，所以城市场景数据集只由漫游的图片组成。这导致了根据空间位置合并共同的结构先验的可能性，特别是在垂直位置。为了验证这一特性，我们在图1中给出了城市场景数据集在垂直位置上的类分布。虽然这几个类别的像素在图像的整个区域中占主导地位(图1(A))，但是类别分布对垂直位置有很大的依赖性。也就是说，图像的下部主要由道路组成，而中间部分包含各种相对较小的对象。在图的上部分中，建筑物、植被和天空是主要的物体，如图1(B)所示。

![image-20200718093627629](D:\MarkDown\DeepLearning\img\image-20200718093627629.png)

图1 我们方法的动机，像素级的类别分类。 所有的数字都是从带有像素级类标注的城市景观数据集的整个训练集中获取的平均值 [12]。请注意，每个图像总共存在2048K像素，y轴为对数刻度。(A)每个条表示分配给单个图像中包含的每个类别的平均像素数。例如，平均每个图像约685k像素被分配给道路类。(B)被分成三个水平部分的图像的每个部分具有彼此显著不同的类别分布。例如，上面的区域只有38个道路类的像素，而下面的区域有480K的像素。

表1显示了占主导地位的前5个类别的概率：道路、建筑、植被、汽车和人行道。类分布极不平衡，占整个数据集的88%以上。如上所述，如果将图像分为三个区域：上部、中部和下部，则类分布完全不同。例如，给出一幅完整的图像，道路等级出现的概率平均为36.9%，但对于较高的区域，这一概率急剧下降到0.006%，而如果考虑到较低的区域，这一概率将跃升至87.9%。

![image-20200718094532699](D:\MarkDown\DeepLearning\img\image-20200718094532699.png)

表1：当整个图像在城市景观数据集的上、中、下区域分开时，分配给每个类别的像素的概率分布(%)的比较。

此外，我们还使用熵(一种衡量不确定性的指标)分析了这一观察结果。城市景观数据集[12]中19个类别上的像素的概率分布X的熵被计算为

![image-20200717174006817](D:\MarkDown\DeepLearning\img\image-20200717174006817.png)

其中$p_i$表示为任意一个像素被指定为第i个类别的概率。条件熵$H(X|img)$是给定一张图片利用等式1计算熵X 为1.84.另外一方面 在上部分，中部分和下部分计算的平均熵为1.26 （见表1）。因此我们可以将图像分割成几个部分，不确定性就会降低。在此基础上，如果能够识别出图像中任意给定像素所属的部分，将有助于语义分割中像素级分类。

基于这些观察结果，我们提出了一种新颖的高度驱动注意力网络(HANET)作为城市场景图像语义分割的通用附加模块。在给定输入特征图的情况下，HANET提取“高度上下文信息”，它表示每个水平划分部分的上下文，然后根据高度上下文信息预测每个水平部分中哪些特征或类别比其他特征或类别更重要。采用HaNet的模型一直比基础模型表现的更好。重要的是，我们提出的模块可以添加到任何基于CNN的骨干网络中，而增加的成本可以忽略不计。为了验证HANET的有效性，我们在ShuffleNetV2[29]、MobileNetV2[37]、ResNet-50[18]和ResNet-101[18]等骨干网络上进行了广泛的实验。我们还将重点放在轻量级主干网络上，其中轻量级HANET更有效。本文的主要贡献包括：

我们提出了一种新的轻量级附加模块，称为HANET，它可以很容易地添加到现有的模型中，并通过根据像素的垂直位置来调整通道的激活来提高性能。通过在不同的骨干网络和两个不同的数据集上进行广泛的实验，我们证明了该方法的有效性和广泛的适用性。

通过将HANET添加到基准中，以可以忽略的计算和存储开销，我们在基于ResNet-101的模型中获得了在城市景观基准上的新的最先进的性能。

我们可视化并解释了各个通道上的注意力权重，以实验验证我们的直觉和理论，即高度位置是提高城市场景分割性能的关键



#### 2、相关工作



**用于语义分割的模型体系结构。**在获取高层语义特征的同时保持特征图的分辨率是实现高性能语义分割的关键。 具有代表性的是通过堆叠多层卷积和空间池化层获取高层特征但是分辨率会在这个过程中降低。一些研究[28,33]利用反卷机使用上采样从低分辨率中获取解除这个限制。Skip-connections通过利用编码层中先前存在的高分辨率功能来恢复解码器层中的对象边界，从而克服了这一限制。另一种流行的方法是Atrus卷积[6，46]，它在不增加参数的情况下增加了接受域的大小，并在最近的语义分割网络[7，8，43，49，53]中被广泛采用。此外，ASPP[7]和金字塔合用模块[51]解决了由不同尺度的对象引起的挑战。

最近，long-range 依赖[54,21, 16] 特别是使用自我注意力机制[39]引入来提高性能表现。关注边界信息是语义分割的另一种方法[3，5，13，38，30]。最近的工作强加了单独的模块，用于目标边界处理[13，38]或边界驱动的自适应下采样[30]。此外，捕获上下文信息也被广泛利用。ACFNet[48]使用类中心来收集每类像素的特征作为分类上下文，而CFNet[50]则学习并发特征的分布并捕获并发上下文以便在做出预测之前进行回顾。CISS-Net[52]采用强化学习来探索预测分割图中的上下文信息，无需任何监督。

**城市景观形象的开发**  在语义分割领域，一些研究利用了城市场景图像的特点。一般来说，在城市场景图像中，对象的比例变化很大。FoveaNet[23]定位了一个“中心凹区域”，在那里小尺度的物体是拥挤的，并且执行尺度归一化来处理不同尺度的物体。DenseASPP[43]采用紧密连接的ASPP，其连接多个ATROS卷积层[20]以处理对象的大规模变化。另一种最近的方法[53]利用城市场景图像具有连续视频帧序列的事实，并提出基于视频预测模型的数据增强技术来创建未来帧及其标签。

自适应领域的最新方法提出了利用城市图像的属性的方法。一种利用空间先验自我学习的类别均衡为没有打标签的目标数据生成伪标签利用哪些类频繁出现在图像中的特定位置以进行无监督的域自适应。另一种方法[10]将城市场景图像划分为几个空间区域，并对来自同一空间区域的像素级特征进行域自适应。同时，利用深度信息和语义之间的相关性，从合成数据中获取城市场景域自适应的附加信息



**Channel-wise attention** 

我们提出的HANet方法与channel-wise有着很强的联系。HANet根据每个channel的开发了特征图的特征和尺寸。压缩激发网络(SENets)[19]使用全局平均池捕获整个图像的全局上下文，并预测每通道的缩放因子，以提取用于图像分类任务的信息特征。该机制在后续的研究中被广泛采用[40,49,44,22]，用于图像分类和语义分割任务。受ParseNet[27]研究的启发，之前的语义分割研究[49,44,22]利用了整个图像的全局上下文来产生信道方向上的注意力。然而，城市场景数据集[12，1，32，4]仅由漫游图片组成，这意味着图像共享相似的类别统计。因此，城市场景图像之间的全局上下文应该时相对相似的。带来的后果就是整个图像的全局上下文不能呈现每个图像的不同信息，以帮助城市场景图像中的每像素分类。这就解释了为什么以前的语义分割的通道注意相关工作主要集中在通用场景数据集上。



#### 3、提出方法

根据空间位置的不同，城市场景图像通常包含共同的结构先验。图像的每一行在类别分布方面具有显著不同的统计数据。在这个意义上，单独捕获表示每行的全局上下文的高度方向上下文信息可以用来估计在城市场景分割的像素级分类期间应该如何对通道进行加权。因此，我们提出了HANET，它的目的是：1)提取高度方向的上下文信息；2)计算高度驱动的注意力权重，以表示特征(在中间层)或类(最后一层)对于每一行的重要性。在这一部分中，我们首先将HANET描述为一个通用的附加模块，然后提出了在不同层次结合多个专门用于城市场景分割的HANET的语义分割网络。

##### 3.1、高度驱动型注意力网络(HANet)

如图2所示，HANet根据其高度相关信息为每个单独的行生成每个通道的比例因子$X_l\in\mathbb{R}^{C_l \times H_l \times W_e}$ 和 $X_h \in \mathbb{R}^{C_h \times H_h \times W_h}$在语义分割网络中代表低级和高级的特征图。C代表通道数，H和W分别代表输入张量的空间高和宽。给定低级特征图$X_l$,$F_{HANET}$生成通道注意力图$A\in \mathbb{R}^{C_h \times H_h}$由每个通道的高度缩放因子组成和拟合到高级特征地图$X_{h}$的通道和高度维度。这是通过一系列步骤来完成的:width wise pooling （图 2a）插入粗糙的注意力（图2（b，d））计算高度驱动的注意图(图2(C)),此外，在该过程中包括添加位置编码(图2(E))。

在计算完attention map后给出高级的特征图$X_h$ 可以被转换到一个新的表示$\tilde{X_{h}}$ .该表示是通过A和$X_{h}$的逐个元素相乘获得的。请注意，针对每个单独行或针对每组多个连续行导出单个每个通道的缩放向量，因此沿水平方向复制向量，其公式如下

![image-20200719095416473](D:\MarkDown\DeepLearning\img\image-20200719095416473.png)

**Width-wise pooling** （图2（a））为了获得channel-wise注意力图，我们首先提取了height-wise上下文信息从每行通过聚合$C_l \times H_l \times W_l$输入表示为$X_{l}$变为$C_l \times H_l \times 1$ 矩阵Z 使用width-wise 池化操作$G_{pool}$

![image-20200719102005544](D:\MarkDown\DeepLearning\img\image-20200719102005544.png)

为了压缩空间有两种经典的池化方法平均池化和最大池化。对于width-wise池化操作 选择最大池化和平均池化之间选择的时平均池化。形式上第h 行向量Z表示为:

![image-20200719102626171](D:\MarkDown\DeepLearning\img\image-20200719102626171.png)

Interpolation for coarse attention (图2（b,d）).在进行池化之后，这个模型生成了一个向量$Z\in \mathbb{R}^{C_l \times H_l}$ 然而，并非矩阵Z的所有行都是计算有效注意力图所必需的。如图1(B)所示，即使我们只将整个区域分成三个部分，每个部分的阶级分布也有很大的不同。因此我们通过下采样即将矩阵Z $C_l \times H_l$插入到$C_l \times \tilde{H}$ 矩阵$\tilde{Z}$.H时一个超参数根据经验设置为16.由于从下采样表示构造的注意图也是粗糙的，attention map被转换到与高级特征图相同高度。如图2（d）。

![image-20200719111736881](D:\MarkDown\DeepLearning\img\image-20200719111736881.png)

图2 我们提供的HANet结构图。每个操作op被标记为$G_{op}$ 特征图为$X_l$ Z为width-wise 池化的$X_l$ $\tilde{Z}$ . $Q^n$第六层的特征图。$\widehat{A}$下采样的attention map，A 最后的attention map。$X_h$高成的特征图。$\tilde{X_h}$ 转换成新的特征图。具体见3.1节。

**高度驱动型注意图的计算** （图2 c）一个大度驱动的channel-wise attention map A 通过利用width-wise池化和interpolated 作为特征图 $\widehat{Z}$卷积获得.最近在分类和语义分割中利用通道方向注意力的工作[19，40，49]采用完全连通层而不是卷积层，因为它们产生对整个图像的通道方向注意力。但是，由于每一行都与其相邻行相关，所以在估计注意图时，我们采用卷积层来考虑相邻行之间的关系。

attention map A指示哪些通道在每个单独的行处是关键的。中间层中的每行可能存在多个信息特征；在最后一层中，每行可以与多个标签(例如，道路、汽车、人行道等)相关联。为了允许这些多个特征和标签，在计算attention map时使用Sigmoid函数，而不是Softmax函数。这些由N个卷积层组成的运算可以写为

![image-20200719135223413](D:\MarkDown\DeepLearning\img\image-20200719135223413.png)

$\sigma$代表sigmoid函数，$\delta$代表ReLU激活函数 $G_{Conv}^i$代表第i个one-dimensional  卷积层。我们根据经验采用三个卷积层：第一层$G_{Conv}^1(\widehat{Z})=Q^1\in \mathbb{R}^{2 \frac{C_l}{r}  \times \widehat{H}}$降低通道。第二层$G_{conv}^2(\delta(Q^1))=Q^2\in \mathbb{R}^{2 \frac{C_l}{r}\times \widehat{H}}$最后一层$G_{conv}^3(\delta(Q^2))=\widehat{A} \in \mathbb{R}^{2 \frac{C_l}{r} \times \widehat{H}}生成attention map。缩减比r降低了HANET的参数开销，并且给出了潜在的正则化效果。不同减速比作为超参数的影响将在第4.2.2节中进行分析。



**Incorporating positional encoding**(图2e)  当人识别驾驶场景时，他们对特定对象的垂直位置具有先验知识(例如，道路和天空分别出现在下部和上部)。受此观察的启发，我们在HANet的第i层特征图$Q^i$添加了正弦位置编码。在补充材料中分析了超参数i。对于注入位置编码，我们遵循Transformer[39]中提出的策略。位置编码的维度与中间特征地图QI的通道维度C相同。位置编码定义为

![image-20200719143456716](D:\MarkDown\DeepLearning\img\image-20200719143456716.png)

其中，p表示整个图像中的垂直位置指数，范围从0到粗略关注的$\widehat{H}$−1，i是维度。垂直位置的数目被设置为$\widehat{H}$，作为粗略关注的行数。包含位置编码的新的表示$\tilde{Q}$

![image-20200719145103290](D:\MarkDown\DeepLearning\img\image-20200719145103290.png)

$\oplus$代表元素相加。

高度位置被随机设置为最多两个的位置以在来自各种数据集的不同相机位置上进行概括，以防止位置-对象耦合异常紧密。此外，我们尝试使用可学习的位置嵌入[17]来找到将位置信息合并到辅助材料中的最佳方式。

同时，CoordConv[26]建议在各种视觉任务的中间特征中嵌入高度和宽度坐标：包含硬编码坐标(例如，高度、宽度和可选r)的额外通道以通道方式连接到输入表示，然后应用标准卷积层。与该模型不同的是，HANET利用高度的位置信息来获得关注值C×H×1，该关注值用于选通主要网络的输出表示。因此，在如何利用位置信息方面，HANET与CoordConv有很大的不同。我们在实验中将我们的与4.2.2节中的CoordConv进行了比较。



##### 3.2 基于HANet的语义分割

我们采用DeepLabv3+[6]作为语义切分的基线。DeepLabv3+具有采用不同扩张率的ASPP编解码器架构。在从骨干网络编码高级表示之后，我们在五个不同层(图3)将HANet添加到分段网络。这是因为较高级别的特征与垂直位置具有更强的相关性。我们进行了一项消融研究，以观察在不同层添加HANET对性能的影响。

##### 3.3  与其他语义分割进行比较

基于自我注意力机制的方法例如DANnet。分别从空间和通道维度的语义依赖关系中获得关注值(H×W)×(H×W)，C×C。然而，HANet并不关注大小之间的关系。HANet包括one-dimension卷积层支配模块化独立设计分支。（图2 a-d）它具体考虑了城市场景数据的结构属性。以此方式，考虑垂直位置，HANET在主网络中的特征地图输出的水平部分导出关注值C×H×1以激活门。HANET比基于自我注意的方法要轻量级得多，后者考虑了每个维度中的每一对之间的关系。同时，像SENET[19]这样的通道式注意方法仅在图像级别产生关注值C×1×1。这对于城市场景分割并不理想，因为大多数图像共享相似的环境和上下文。

![image-20200719154352550](D:\MarkDown\DeepLearning\img\image-20200719154352550.png)

图3 在语义分割网络中的5层网络中插入HANet。



#### 4、实验

在本节中，我们首先描述了HANet的实现细节。然后我们通过大量的定量分析，包括ablation研究，从实验上证明了哦我们提出的方法的有效性和广泛适用性。我们验证了HANet在两个不同的城市景观数据集Cityspace和BDD100k.此外，我们还对HANET生成的注意图进行了可视化和分析。对于所有的定量实验，我们采用mIoU来衡量分割性能。



##### 4.1 实验装置

**基本分割结构**  我们的语义分割网络架构基于DeepLabv3+。我们采用了包括ShuffleNetV2[29]、MobileNetV2[37]和ResNet[18]在内的各种骨干网络来验证HANET的广泛适用性。请注意，HANET可以很容易地插入到各种主干网络的顶部。除非另有说明，所采用的主干网络在ImageNet[36]上针对所有实验进行了预训练。

**更强的baseline**  为了严格验证HANET的有效性，我们采用ResNet-101改进了DeepLabv3+baseline的性能，通过继承SyncBatch Norm(跨多个GPU同步的批处理统计信息)。公开包含在PyTorch v1.1中，并将ResNet-101第一层中的单个7×7卷积替换为三个3×3卷积。我们还在中间特征映射中采用了辅助交叉点损失和类均匀采样[35，53]来处理类不平衡问题。结果表明，我们的baseline在Cityscapes验证集上的mIoU达到了79.25%，超过了前人使用ResNet-101基于DeepLabv3+架构的baseline模型。

**训练方案**

我们使用SGD优化器，初始学习率为1e-2，动量为0.9。主网和HANET的权重衰减分别为5e-4和1e-4。学习速率调度遵循多项式学习速率策略[27]。初始学习速率乘以$(1- \frac{iteration}{max_iteration}^{power})$ power为0.9 .为了避免过拟合，语义图像分割模型中使用了典型的数据增强，包括随机水平翻转、[0.5，2]范围内的随机缩放、高斯模糊、颜色抖动和随机裁剪。

##### 4.2 Cityscape数据集

Cityscape数据集[12]是一个大规模的城市场景数据集，包含5K图像和20K粗注释图像的高质量像素级注释。精细注释的图像由2975个列车图像、500个验证图像和1525个测试图像组成。测试图像的注释将保留用于基准测试。每幅图像的分辨率为2048×1024，定义了19个语义标签。在所有关于城市景观验证集的实验中，我们使用经过精细标注的训练集对我们的模型进行了40K次迭代，总批次为8，裁剪大小为768×768。



###### 4.2.1 HANet的有效性

表2 展示了采用HANet的印象通过mIou的增加 参数和FLOPs分别表示模型的大小和复杂性。为了证明HANET的广泛适用性，我们检查了各种主干，包括ShuffleNetV2[29]、MobileNetV2[37]、ResNet-50和ResNet101[18]。配备HANET的机型始终优于基准机型，在MobileNetV2和ResNet-101上有显著增长。此外，模型参数和复杂度结果表明，添加HANET的代价几乎可以忽略不计。从图4中，我们可以清楚地看到，与通过改变输出步幅(红色箭头)来改进模型相比，添加HANET(蓝色箭头)比在FLOPS中增加成本更有价值。因此，HANET不仅是提高语义分词准确率的有效途径，而且算法设计轻量级，便于实际应用。

![image-20200719170608276](D:\MarkDown\DeepLearning\img\image-20200719170608276.png)

![image-20200719173520036](D:\MarkDown\DeepLearning\img\image-20200719173520036.png)