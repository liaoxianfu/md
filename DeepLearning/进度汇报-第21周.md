本周阅读论文《Cars Can’t Fly up in the Sky: Improving Urban-Scene Segmentation via
Height-driven Attention Networks》 本文主要是针对城市图像语义分割进行优化。根据城市景观图像的特性将每张图片分割为上中下三部分，提出了HANet可以集成到现有的语义化分割网络中，而且HANet是轻量化网络计算代价可以忽略不计。



### 摘要

论文利用城市场景图像的内在特征，提出了一种通用的高度驱动注意力网络(HANet)用于改进城市场景图像的语义分割。它根据垂直像素的位置强调信息性的特征或有选择的类别。城市场景图像中水平分段段之间的像素级分布明显不同。同样，城市场景图像也有其鲜明的特点，但是大多数语义分割网络并没有在体系结构中反映出这种独特的属性。作者提出的网络结构包括了这种属性来有效处理城市数据集的能力。



#### 1、介绍

作者首先介绍了语义分割在自动驾驶上的重要性，并介绍了不同的语义分割方法，从最开始使用完全卷积网络FCNs的开创性工作，到使用编码-解码技术和skip-Connections，atrous卷积，ASPP网络和金字塔池化模型等计算。

接着介绍了城市景观图的特有特性:由于城市场景图像是由安装在汽车前侧的摄像头拍摄的，所以城市场景数据集只由漫游的图片组成。这导致了根据空间位置合并共同的结构先验的可能性，特别是在垂直位置。作者为了验证这一特性，对数据集进行了统计 （如下图所示）从统计图中可以得到结论：(A)每个条表示分配给单个图像中包含的每个类别的平均像素数。例如，平均每个图像约685k像素被分配给道路类。(B)被分成三个水平部分的图像的每个部分具有彼此显著不同的类别分布。。

![image-20200718093627629](D:\MarkDown\DeepLearning\img\image-20200718093627629.png)

表1显示了占主导地位的前5个类别的概率：道路、建筑、植被、汽车和人行道。类分布极不平衡，占整个数据集的88%以上。如上所述，如果将图像分为三个区域：上部、中部和下部，则类分布完全不同。例如，给出一幅完整的图像，道路等级出现的概率平均为36.9%，但对于较高的区域，这一概率急剧下降到0.006%，而如果考虑到较低的区域，这一概率将跃升至87.9%。

表1 

![image-20200718094532699](D:\MarkDown\DeepLearning\img\image-20200718094532699.png)



作者还使用熵(一种衡量不确定性的指标)分析了这一观察结果。城市景观数据集[12]中19个类别上的像素的概率分布X的熵被计算为

![image-20200717174006817](D:\MarkDown\DeepLearning\img\image-20200717174006817.png)

其中$p_i$表示为任意一个像素被指定为第i个类别的概率。条件熵$H(X|img)$是给定一张图片利用等式1计算熵X 为1.84.另外一方面 在上部分，中部分和下部分计算的平均熵为1.26 （见表1）。因此我们可以将图像分割成几个部分，不确定性就会降低。



基于上述分析，作者提出了一种新颖的高度驱动注意力网络(HANET)作为城市场景图像语义分割的通用附加模块。在给定输入特征图的情况下，HANET提取“高度上下文信息”，它表示每个水平划分部分的上下文，然后根据高度上下文信息预测每个水平部分中哪些特征或类别比其他特征或类别更重要。



#### 2、相关工作

作者介绍了语义分割的体系结构和发展阶段。接着介绍了城市景观数据集的开发,在之前工作中对于城市景观的处理方法。例如FoveaNet定位了一个“中心凹区域”，在那里小尺度的物体是拥挤的，并且执行尺度归一化来处理不同尺度的物体。DenseASPP采用紧密连接的ASPP，其连接多个ATROS卷积层以处理对象的大规模变化。另一种最近的方法利用城市场景图像具有连续视频帧序列的事实，并提出基于视频预测模型的数据增强技术来创建未来帧及其标签。

最后介绍了Channel-wise attention。HANet根据每个channel的开发了特征图的特征和尺寸。压缩激发网络(SENets)[19]使用全局平均池捕获整个图像的全局上下文，并预测每通道的缩放因子，以提取用于图像分类任务的信息特征。



#### 3、提出方法

根据空间位置的不同，城市场景图像通常包含共同的结构先验。图像的每一行在类别分布方面具有显著不同的统计数据。在这个意义上，单独捕获表示每行的全局上下文的高度方向上下文信息可以用来估计在城市场景分割的像素级分类期间应该如何对通道进行加权。因此作者提出了HANet网络，目的是1)提取高度方向的上下文信息；2)计算高度驱动的注意力权重，以表示特征(在中间层)或类(最后一层)对于每一行的重要性。

##### 3.1、高度驱动型注意力网络(HANet)

如图2所示，HANet根据其高度相关信息为每个单独的行生成每个通道的比例因子$X_l\in\mathbb{R}^{C_l \times H_l \times W_e}$ 和 $X_h \in \mathbb{R}^{C_h \times H_h \times W_h}$在语义分割网络中代表低级和高级的特征图。C代表通道数，H和W分别代表输入张量的空间高和宽。给定低级特征图$X_l$,$F_{HANET}$生成通道注意力图$A\in \mathbb{R}^{C_h \times H_h}$由每个通道的高度缩放因子组成和拟合到高级特征图$X_{h}$的通道和高度维度。

![image-20200719111736881](D:\MarkDown\DeepLearning\img\image-20200719111736881.png)

在计算完attention map后给出高级的特征图$X_h$ 可以被转换到一个新的表示$\tilde{X_{h}}$ .该表示是通过A和$X_{h}$的逐个元素相乘获得的。请注意，针对每个单独行或针对每组多个连续行导出单个每个通道的缩放向量，因此沿水平方向复制向量，其公式如下

![image-20200719095416473](D:\MarkDown\DeepLearning\img\image-20200719095416473.png)

**Width-wise pooling** （图2（a））为了获得channel-wise注意力图，我们首先提取了height-wise上下文信息从每行通过聚合$C_l \times H_l \times W_l$输入表示为$X_{l}$变为$C_l \times H_l \times 1$ 矩阵Z 使用width-wise 池化操作$G_{pool}$

![image-20200719102005544](D:\MarkDown\DeepLearning\img\image-20200719102005544.png)

为了压缩空间有两种经典的池化方法平均池化和最大池化。对于width-wise池化操作 选择最大池化和平均池化之间选择的时平均池化。形式上第h 行向量Z表示为:

![image-20200719102626171](D:\MarkDown\DeepLearning\img\image-20200719102626171.png)

Interpolation for coarse attention (图2（b,d）).在进行池化之后，这个模型生成了一个向量$Z\in \mathbb{R}^{C_l \times H_l}$ 然而，并非矩阵Z的所有行都是计算有效注意力图所必需的。如图1(B)所示，即使我们只将整个区域分成三个部分，每个部分的阶级分布也有很大的不同。因此我们通过下采样即将矩阵Z $C_l \times H_l$插入到$C_l \times \tilde{H}$ 矩阵$\tilde{Z}$.H时一个超参数根据经验设置为16.由于从下采样表示构造的注意图也是粗糙的，attention map被转换到与高级特征图相同高度。如图2（d）。

**高度驱动型注意图的计算** （图2 c）一个大度驱动的channel-wise attention map A 通过利用width-wise池化和interpolated 作为特征图 $\widehat{Z}$卷积获得.attention map A指示哪些通道在每个单独的行处是关键的。中间层中的每行可能存在多个信息特征；在最后一层中，每行可以与多个标签(例如，道路、汽车、人行道等)相关联。为了允许这些多个特征和标签，在计算attention map时使用Sigmoid函数，而不是Softmax函数。这些由N个卷积层组成的运算可以写为

![image-20200719135223413](D:\MarkDown\DeepLearning\img\image-20200719135223413.png)

**Incorporating positional encoding**(图2e)  当人识别驾驶场景时，他们对特定对象的垂直位置具有先验知识(例如，道路和天空分别出现在下部和上部)。受此观察的启发，作者在HANet的第i层特征图$Q^i$添加了正弦位置编码。

![image-20200719143456716](D:\MarkDown\DeepLearning\img\image-20200719143456716.png)

其中，p表示整个图像中的垂直位置指数，范围从0到粗略关注的$\widehat{H}$−1，i是维度。垂直位置的数目被设置为$\widehat{H}$，作为粗略关注的行数。包含位置编码的新的表示$\tilde{Q}$

![image-20200719145103290](D:\MarkDown\DeepLearning\img\image-20200719145103290.png)

$\oplus$代表元素相加。

##### 3.2 基于HANet的语义分割

作者采用DeepLabv3+[6]作为语义切分的基线。DeepLabv3+具有采用不同扩张率的ASPP编解码器架构。在从骨干网络编码高级表示之后，在五个不同层(图3)将HANet添加到分段网络。这是因为较高级别的特征与垂直位置具有更强的相关性。作者进行了一项消融研究，以观察在不同层添加HANET对性能的影响。

![image-20200719154352550](D:\MarkDown\DeepLearning\img\image-20200719154352550.png)

#### 4、实验

##### 4.1 实验装置

**基本分割结构**  语义分割网络架构基于DeepLabv3+。采用了包括ShuffleNetV2、MobileNetV2和ResNet在内的各种骨干网络来验证HANET的广泛适用性。HANET可以很容易地插入到各种主干网络的顶部。



**更强的baseline**  为了严格验证HANet的有效性，采用ResNet-101改进了DeepLabv3+baseline的性能，通过继承SyncBatch Norm，将ResNet-101第一层中的单个7×7卷积替换为三个3×3卷积。作者还在中间特征映射中采用了辅助交叉点损失和类均匀采样来处理类不平衡问题。结果表明，HANet的baseline在Cityscapes验证集上的mIoU达到了79.25%，超过了前人使用ResNet-101基于DeepLabv3+架构的baseline模型。



**训练方案**

HANet使用SGD优化器，初始学习率为1e-2，动量为0.9。主网和HANET的权重衰减分别为5e-4和1e-4。学习速率调度遵循多项式学习速率策略[27]。初始学习速率乘以$(1- \frac{iteration}{max_iteration}^{power})$ power为0.9 .为了避免过拟合，语义图像分割模型中使用了典型的数据增强，包括随机水平翻转、[0.5，2]范围内的随机缩放、高斯模糊、颜色抖动和随机裁剪。



###### 4.2.1 HANet的有效性

表2 展示了采用HANet的印象通过mIou的增加 参数和FLOPs分别表示模型的大小和复杂性。为了证明HANet的广泛适用性，作者实验了各种网络，包括ShuffleNetV2、MobileNetV2、ResNet-50和ResNet101。配备HANET的始终优于基准，在MobileNetV2和ResNet-101上有显著增长。此外，模型参数和复杂度结果表明，添加HANET的代价几乎可以忽略不计。从图4中，与通过改变输出步幅(红色箭头)来改进模型相比，添加HANET(蓝色箭头)比在FLOPS中增加成本更有价值。因此，HANET不仅是提高语义分词准确率的有效途径，而且算法设计轻量级，便于实际应用。

![image-20200719170608276](D:\MarkDown\DeepLearning\img\image-20200719170608276.png)

![image-20200719173520036](D:\MarkDown\DeepLearning\img\image-20200719173520036.png)

