本周阅读论文《Learning Dynamic Routing for Semantic Segmentation》，其主要思想是将动态网络引入语义化分割并提出了资源预算用于降低计算成本并让网络根据不同的资源预算自动训练出适合的网络模型。



[TOC]

## 摘要

​	以前的语义分割工作大多都是在预定义的静态体系结构中（如FCN、U-Net和DeepLab系列)中处理各种尺度的输入。这篇论文提出了一种缓解语义表示尺度差异的方法--动态路由。这种框架生成依赖于数据的路由，以适应每幅图像的尺度分布。此外，通过预先设置门控函数的约束条件可以使用端到端的方式进一步的降低计算成本并且进一步放宽了网络级路由空间，以支持每次转发中的多路径传播和跳过连接，从而带来可观的网络容量。作者对比了一些静态体系结构并在Cityscape和Pascal VOC 2012上进行了大量的实验，以说明动态框架的有效性。



## 1、介绍

​	作者首先介绍了语义分割的定义“其目的是为每个像素赋予语义类别”和语义分割目前存在的问题之一是"来自于输入之间巨大的尺度差异"。传统的方法试图通过设计良好的网络体系结构来解决这一问题。例如多分辨率融合采用针对细节的feature maps和long-range dependencies 被捕获通过全局上下文建模以及随着NAS网络的发展也开始用于语义分割。然而，不管是传统的基于人工设计的还是基于NAS的网络都只是试图用一个单一的网络结构表现所有实例。这样缺乏对真实环境中不同尺寸分布表现的适应性。

​	在这篇论文中，作者对于语义分割提出了一个概念意义上新颖的模型，称之为动态路由。动态路由在推理过程中生成依赖于数据的转发路径，这意味着特定的网络结构会随着输入的不同而不同。通过这种方法我们使用自定义的特征转换可以将不同尺度的实例（或背景）分配到相应的分辨率阶段。该网络与AutoDeepLab不同的是多路径传播和跳过连接被证明在语义分割中是非常重要的，在推理过程中的每一次转发中都启用了多路径传播和跳过连接。因此几种经典的网络体系结构可以作为比较的特例包括在内并且在动态路由中，设计了一种与数据相关的路由门，被称为软控制门，被设计为根据输入图像选择每条路径。



## 2、相关工作

传统的语义分割研究主要集中在根据人的经验设计微妙的网络体系结构。随着NAS网络的发展有许多的网络尝试自动进行搜索静态网络。与以往的工作不同的是，动态路由是根据输入选择最合适的尺度变换。



## 3、学习动态路由

与静态网络相比，动态路由在资源消耗相同的情况下在网络容量和性能上具有优势。

### 3.1、路由空间

为了释放动态路由的潜力，我们在相邻层之间提供具有一些先验约束的全连接路径，例如，单元之间的上采样或下采样跨度，如图所示。网络的开始是一个固定的3层“STEM”块，将分辨率降低到1/4尺度。在此基础上，设计了L层动态路由空间，称为路由空间。在路由空间中，相邻的Cell之间的比例因子被限制为2，这在基于RestNet的方法中被广泛采用。因此，最小比例设置为1/32。在这些约束条件下，每层候选的数目最多为4个（1/4 1/8 1/16 和1/32），每个候选图像有3条尺度变换路径，即上采样、保持分辨率和下采样。

与Auto-DeepLab每个节点只选择一条特定的路线不同的是动态路由进一步放宽了路由空间，支持每个候选节点多路径路由和跳过连接。

![image-20200703104512327](D:\MarkDown\DeepLearning\img\image-20200703104512327.png)



### 3.2、路由处理

​	具体就是先聚合来自$l-1$层具有不同空间大小的三个输入(也就是 $s/2、s、2s$) 分别表示为$Y_{s/2}^{l-1}、Y_{s}^{l-1}、Y_{2s}^{l-1}$因此,可以将第l层的输入$X_s^l$表示为
$$
X_s^l=Y_{s/2}^{l-1}+Y_{s}^{l-1}+Y_{2s}^{l-1}
$$
然后聚合的输入将用于$Cell$ $Gate$的内部转换.见上图右半部分。



#### 3.2.1 操作cell

对于输入数据$X_s^l\in \mathbb{R}^{B*C*W*H}$,作者在每个单元中采用了广泛使用的分离卷积的堆栈和恒等式映射,,隐藏状态$H_s^l\in\mathbb{R}^{B*C*W*H}$可以表示为
$$
H_s^l=\sum_{O^i\in{o}}O^i(X_s^l)\quad\quad\quad\quad\quad\quad\quad   (1)
$$
o代表可操作集合,包括SepConv3x3和identity mapping.这里对每个cell采用基本的特征融合. 然后会生成特征图$H_s^{l}$将根据激活因子$\alpha_s^l$变换到不同的尺度.

### 3.3.2 Soft Conditional Gate 软控门

每条路径的路由概率由门控函数生成，门控函数采用轻量化卷积运算来学习依赖数据的向量$G_s^l$ 公式如下:

![image-20200703171059084](D:\MarkDown\DeepLearning\img\image-20200703171059084.png)

其中F(·，·)表示卷积函数，σ表示RELU激活，N和G分别表示 batch normalization 和全局平均池化 (global average pooling )。ω和β都是卷积参数。与传统的基于RL的方法采用策略梯度更新代理进行离散路径选择不同，本文提出了可区分路由的软控门,为此,对于特征向量$G_s^l\in\mathbb{R}^{B*3*1*1}$激活函数被设计为
$$
\delta(.)=max(0,Tanh(.)) \quad \quad (3)
$$
因此,激活因子$\alpha_s^l\in\mathbb{R}^{B*3*1*1}$可以通过$\delta(G_s^l)$计算出来,$\alpha_s^l$区间为[0,1)当$\alpha_{s\to j}^l=0$时从s->j的路径就会被关闭.所有$\alpha_{s \to j}^l>0$的路径都会被保留下来,允许多路径传播. 更具体的来说,在第B批次中的第b个输入将会生成$\alpha_{b,s\to j}^l\in \mathbb{R}^{1*1*1*1}$ 这意味着路由路径随输入或所谓的数据相关而变化。这样,每条路径都可以被单独考虑而不是选择一条相对重要的路径进行传播。

在推理阶段,如果所有的路径都被标记为关闭,Cell中的操作会被丢弃来节省计算空间,总的门控函数如下:
$$
Y_j^l = \alpha_{s\to j}^l \Tau_{s\to j}(H_s)^l \quad \quad (4)
$$
![image-20200703183605488](D:\MarkDown\DeepLearning\img\image-20200703183605488.png)

### 3.3 预算约束

考虑到现实场景中有限的计算资源，论文考虑了预算约束，以实现高效的动态路由。作者定义$C$作为与预定义操作 相关联的计算成本例如 FLOPs.根据等式1,2和4 我们表示在第s层的尺寸和第l层的layer预期的花费为:

![image-20200703184500173](D:\MarkDown\DeepLearning\img\image-20200703184500173.png)

其中$Cell_s^l \quad Gate_s^l \quad Trans_s^l$分别表示Cell、Gate、Scale内部变换的操作、更进一步，整个路由空间可以通过下式进行计算
$$
C(Space)=\sum_{l\le L}\sum_{s\le1/4}C(Node_s^l) \quad\quad (8)
$$
然后将资源花费C(Space)在loss函数中进行表示 进行端到端的优化。

![image-20200703185204461](D:\MarkDown\DeepLearning\img\image-20200703185204461.png)

用C表示整个路由空间真实的计算成本，µ∈[0，1]表示设计衰减系数。使用不同的µ，每次传播中选择的路由将自适应地限制在相应的预算内。总体而言，网络权重和软控门能够在统一的框架下使用联合损失函数L进行优化。

![image-20200703185756006](D:\MarkDown\DeepLearning\img\image-20200703185756006.png)

其中$L_N和L_C$分别表示整个网络的算是函数和资源花费，λ1和λ2分别用于平衡网络预测和资源成本预期的优化过程。

### 3.4 网络的详细结构

动态路由网络将空间深度设置为16或33，与广泛使用的ResNet-50和ResNet-101中的相同，即上图中的总层L=16或33。与基于RestNet的网络相比，这种设置能够带来便利，网络可以直接使用建议的路由空间进行构建。

当在网络中涉及到微节点的时候，在STEM块中采用3个SepConv3x3，这些卷积中共计有64个filter,对所有s->s/2的路径使用一个步长为2的1x1卷积核进行卷积，即降低了特征分辨率又将filter翻倍。在s->2s连接上使用1x1卷积核进行双线性上采样，即可以提高分辨率又可以减少filter

此外，设计了一个简单的解码器来融合最终预测的特征，在上图中，最终预测表示为网络末端的灰色节点。具体来说就是在解码器中使用带有双线性上采样的1*1卷积融合不同的尺寸。并且将尺度为1/4预测进行上采样生成最终的结果。卷积中的权重采用正太分布进行初始化，等式2中的偏置$\beta_s^l$通过实现设置为常数1.5。当给定预先约束，我们将等式2中的输入$X_s^l$ 通过4次减少资源消耗的门控函数进行下采样。除此之外，输入$X_s^l$的分辨率保持不变。

## 4、 实验

作者在 Cityscapes  和 PASCAL VOC 2012 两个数据集上与一些网络进行了比较，证明了该网络的有效性和高效性。

### 4.1 数据集

介绍了Cityscapes  和 PASCAL VOC 2012 两个数据集。

### 4.2 实现细节

为了便于复现，论文报告了优化的细节信息。为了获得更好的表现，等式10的$\lambda_1$设为1.0 ,$\lambda_2$根据4.4.3节中不同的约束分类进行设置。网络优化采用SGD，衰减为$1e^{-4}$动量为0.9.采用‘ploy’计划 初始化速率乘以$(1-\frac{iter}{iter_{max}})^{power}$ 在每次迭代的时候设置$power$为0.9。在训练阶段，随机反转和进行0.5到倍的缩放。根据实验设置施加不同的初始速率。具体地说，在从头开始训练和使用ImageNet进行预训练时分别将初始率设置为0.05和0.02。对于Cityscapes从8个随机的768×768个图像作物中构造每个小批次用于训练。对于Pascal VOC 2012在每次迭代中采用16个随机的512×512图像裁剪进行优化。

### 4.3 动态路由

为了证明动态路由的优越性，作者将动态网络与一些静态网络进行了比较。在相同代价下动态路由获得了更好的性能。例如，给定45G、55G和65G左右的预算约束，动态A、B和C分别比DeepLabV3、U-Net和HRNetV2获得5.8%、2.2%和2.1%的增加。

![image-20200704170211029](D:\MarkDown\DeepLearning\img\image-20200704170211029.png)

作者观察到通用网络连接与已知的几种体系结构相似，特别是在网络的前端进行下采样操作在后端进行上采样操作会获得更好的性能表现。如下图所示。

![image-20200701182832150](D:\MarkDown\DeepLearning\img\image-20200701182832150.png)

### 4.4 组件详细分析

为了揭示所提出的方法中每个组件的作用，在本节中我们将一步步的分解我们的方法。

#### 4.4.1 Cell组件

了与以前的网络结构进行公平比较，每个cell中只有基本的卷积操作和identity mapping，没有其他华而不实的操作。下表是与一些经典操作包括（ BottleNeck [, MBConv 和SepConv  ）的实验结果。我们发现动态网络在堆叠两个SepConv3×3进行特征变换时性能最好，heavier operations不会带来更多的增益。作者猜测这可能归结于路由结构相比于heavier operations扮演着更重要的角色。

![image-20200704174504013](D:\MarkDown\DeepLearning\img\image-20200704174504013.png)

#### 4.4.2 激活函数

论文中自己实现的激活函数相比较与其他的普通激活函数效果更好。

![image-20200704174517941](D:\MarkDown\DeepLearning\img\image-20200704174517941.png)

#### 4.4.3 资源预算

通过设计门控函数，调整$\lambda_2$和$\mu$来提供不同的资源预算。如下表所示。在性能损失不大的情况下Dynamic-C ，计算开销降低了**55.7%**同时Dynamic-C 在有效性和效率上仍然优于全连接的Network-Fix。在更强的约束下，资源开销在Dynamic-A中能否进一步下降到37.6%。

![image-20200704180333902](D:\MarkDown\DeepLearning\img\image-20200704180333902.png)

路由激活概率如下所示，从下图可以明显看出大多数路径倾向于保留在Dynamic-Raw中。如果给定资源预算，将丢弃不同比例的路线，也就是动态路由会在推理过程中剔除无用的路径和cell。

![image-20200704201408551](D:\MarkDown\DeepLearning\img\image-20200704201408551.png)

### 4.5 在Cityscapes数据集上实验

仅使用了精确注释的Cityspace数据集。在下表比较了几个训练好的数据集在验证集上。所提出的方法在不同的情况下取得了一致的改进效果。使用Scheduled Drop Path和ImageNet预训练可以进一步的提高动态网络（L=16）的性能。

![image-20200704201743729](D:\MarkDown\DeepLearning\img\image-20200704201743729.png)



在资源开销相近的情况下，提出的动态网络在验证集上达到了78.6%的mIoU比精心设计的BiSenet提高了3.8%。动态网络(L=33)采用简单的比例变换没有华而不实的操作，性能与最先进的网络相当但是资源消耗成本要低的多。此外，结合上下文捕获模块（如PSP块），该方法有了进一步的改进，在Cityspace数据集上取得了80.7%的mIoU。

![image-20200704202146969](D:\MarkDown\DeepLearning\img\image-20200704202146969.png)

### 4.6 在PASCAL  VOC数据集上实验

在准确性和效率上都超过了Auto-DeepLab,后者需要花费3GPUdays在architecture searching，如下表所示。与基于MobileNet的DeepLabV3相比，动态网络使用相似的资源花费得到了更好的表现。

![image-20200704202816329](D:\MarkDown\DeepLearning\img\image-20200704202816329.png)