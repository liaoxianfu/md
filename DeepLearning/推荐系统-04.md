本周阅读facebook提出的DLRM网络模型论文《Deep Learning Recommendation Model for Personalization and Recommendation Systems》这是Facebook在工业级实践使用的网络模型。





### 摘要

随着深度学习的到来，基于神经网络的推荐模型已经成为解决个性化和推荐任务的重要工具。这些网络与其他深度学习网络有很大的不同，因为它们需要处理范畴特征，并且没有得到很好的研究或理解。在本文中，作者开发了一个最新的深度学习推荐模型(DLRM)，并给出了它在PyTorch和Caffe2框架中的实现。此外，作者设计了一种专门的并行化方案，利用嵌入表上的模型并行性来缓解内存限制，同时利用数据并行性从完全连接层向外扩展计算。作者将DLRM与现有的推荐模型进行了比较，并在大流域AI平台上对其性能进行了表征，证明了它作为未来算法实验和系统协同设计的基准的有效性。



### 介绍

目前，在大型互联网公司，个性化和推荐系统被部署用于各种任务，包括广告点击率(CTR)预测和排名。虽然这些方法已经有了很长的历史，但这些方法直到最近才开始采用神经网络。针对个性化和推荐的深度学习模型的架构设计有两个主要视角。

第一个是从推荐系统的角度。这些系统最初采用内容过滤，一组专家将产品分类成类别，而用户选择他们喜欢的类别，并根据他们的喜好进行匹配。该领域随后演变为使用协作过滤，其中推荐基于过去的用户行为，例如之前给予产品的评级。通过将用户和产品分组在一起来提供推荐的邻域方法和通过矩阵因式分解技术通过某些隐含因素来表征用户和产品的潜在因素方法后来被成功地部署。



第二种观点来自预测分析，它依赖统计模型根据给定的数据对事件的概率进行分类或预测。预测模型从使用简单的模型，如线性和逻辑回归转变为包含深层网络的模型。为了处理分类数据，这些模型采用了嵌入的方法，将一个或多个热点向量转化为抽象空间中的稠密表示。这个抽象空间可以解释为推荐系统发现的潜在因素的空间。



在本文中，作者介绍了一个由上述两种观点结合而成的个性化模型。该模型使用嵌入来处理表示分类数据的稀疏特征，使用多层感知器(MLP)来处理密集特征，然后使用[24]中提出的统计技术显式地交互这些特征。最后，通过对与另一个MLP的交互进行后处理，得到事件概率。作者将这个模型称为深度学习推荐模型(DLRM)；见图1。该模型的PyTorch和Caffe2实现将被发布，用于测试和实验本手稿的出版。



### 模型设计与结构

![image-20201018171544862](D:\MarkDown\DeepLearning\img\image-20201018171544862.png)

本文所设计的推荐系统架构如 上图 所示。整个模型主要包含：特征工程（包含 spare 和 dense 特征），用于特征建模 Embedding 和 Embedding Lookup，用于特征转换的 NNs，用于特征交互的 Interactions 以及最后的预测 NNs。





#### **特征表示 Embedding**

在实际的推荐场景中，用户和商品通常都有丰富的特征信息。用户的特征通常用性别，年龄，居住地等。如何将这些类别特征转为模型可以处理的向量呢。本文的做法是将这些类别特征编码为 one-hot 的向量，然后通过 embedding lookup 来得到其表示。


$$
W_i^T = e_iTW
$$


以用户的性别为例，性别男的 one-hot 编码为 [1, 0] 性别女的 one-hot 编码为 [0,1]。然后，作者针对性别初始化一个关于性别的 embedding matrix，该矩阵大小为 2*d，2 代表性别的可能取值，d 代表 embedding 的维度。那么通过 embedding lookup，性别男的 embedding 其实就是 embedding matrix的第一行，性别女的 embedding 就是 embedding matrix 的第二行。通过上述操作，作者就将难以处理的类别特征转化为了神经网络方便处理的向量。



上述过程得到是类别特征的初始 embedding，作者可以通过 MLP 对其进行非线性转换。初始的特征 embedding 会在模型优化过程中学习到具有区分度的特征表示。



#### 特征交互 Interaction

在得到特征的表示后，作者通过内积等简单操作实现模型的预测：
$$
min \sum_{(i,j)\in S} r_{i,j} - w_i ^Tv_j
$$


#### **模型预测 NNs**



有了特征的表示及其交互之后，作者可以将其送入到 MLP 中，并利用 Sigmoid 函数预测最终的点击概率。

![img](https://pic4.zhimg.com/80/v2-8bcf04be227affc4a1763fcde2a310e3_720w.png)



#### **模型并行**

在工业界的大规模数据下，模型并行是必不可少的一个步骤。DLRM 模型的主要参数来自于特征的 embedding，后面特征交互和模型预测部分的参数其实很少。假设作者有一亿个用户，如果对其 ID 进行 embedding，那么 embedding matrix 就会有一亿行，这是一个非常大的参数矩阵。



对于特征 embedding 部分，这里采用的是模型并行，将一个大的embedding 矩阵放到多个设备上，然后更新相应的特征 embedding。



对于特征交互和模型预测部分，这里的参数量相对较少而且用户/商品的数量无关，本文采用的是数据并行的方式。在多个设备上计算梯度，然后将梯度合并来更新模型。



代码实现

emb 网络

```python
    def create_emb(self, m, ln):
        emb_l = nn.ModuleList()
        for i in range(0, ln.size):
            n = ln[i]
            # construct embedding operator
            if self.qr_flag and n > self.qr_threshold:
                EE = QREmbeddingBag(n, m, self.qr_collisions,
                    operation=self.qr_operation, mode="sum", sparse=True)
            elif self.md_flag:
                base = max(m)
                _m = m[i] if n > self.md_threshold else base
                EE = PrEmbeddingBag(n, _m, base)
                # use np initialization as below for consistency...
                W = np.random.uniform(
                    low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, _m)
                ).astype(np.float32)
                EE.embs.weight.data = torch.tensor(W, requires_grad=True)

            else:
                EE = nn.EmbeddingBag(n, m, mode="sum", sparse=True)

                W = np.random.uniform(
                    low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)
                ).astype(np.float32)
                # approach 1
                EE.weight.data = torch.tensor(W, requires_grad=True)


            emb_l.append(EE)

        return emb_l
```

mlp网络

```python
    def create_mlp(self, ln, sigmoid_layer):
        # build MLP layer by layer
        layers = nn.ModuleList()
        for i in range(0, ln.size - 1):
            n = ln[i]
            m = ln[i + 1]


            LL = nn.Linear(int(n), int(m), bias=True)

            mean = 0.0  # std_dev = np.sqrt(variance)
            std_dev = np.sqrt(2 / (m + n))  # np.sqrt(1 / m) # np.sqrt(1 / n)
            W = np.random.normal(mean, std_dev, size=(m, n)).astype(np.float32)
            std_dev = np.sqrt(1 / m)  # np.sqrt(2 / (m + 1))
            bt = np.random.normal(mean, std_dev, size=m).astype(np.float32)
            # approach 1
            LL.weight.data = torch.tensor(W, requires_grad=True)
            LL.bias.data = torch.tensor(bt, requires_grad=True)

            layers.append(LL)

            # construct sigmoid or relu operator
            if i == sigmoid_layer:
                layers.append(nn.Sigmoid())
            else:
                layers.append(nn.ReLU())

        return torch.nn.Sequential(*layers)

```

![image-20201018172815038](D:\MarkDown\DeepLearning\img\image-20201018172815038.png)