## è‡ªåŠ¨æ±‚å¯¼



PyTorchåœ¨autogradæ¨¡å—ä¸­å®ç°äº†è®¡ç®—å›¾çš„ç›¸å…³åŠŸèƒ½ï¼Œautogradä¸­çš„æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯Variableã€‚ä»v0.4ç‰ˆæœ¬èµ·ï¼ŒVariableå’ŒTensoråˆå¹¶ã€‚æˆ‘ä»¬å¯ä»¥è®¤ä¸ºéœ€è¦æ±‚å¯¼(requires_grad)çš„tensorå³Variable. autogradè®°å½•å¯¹tensorçš„æ“ä½œè®°å½•ç”¨æ¥æ„å»ºè®¡ç®—å›¾ã€‚

Variableæä¾›äº†å¤§éƒ¨åˆ†tensoræ”¯æŒçš„å‡½æ•°ï¼Œä½†å…¶ä¸æ”¯æŒéƒ¨åˆ†`inplace`å‡½æ•°ï¼Œå› è¿™äº›å‡½æ•°ä¼šä¿®æ”¹tensorè‡ªèº«ï¼Œè€Œåœ¨åå‘ä¼ æ’­ä¸­ï¼Œvariableéœ€è¦ç¼“å­˜åŸæ¥çš„tensoræ¥è®¡ç®—åå‘ä¼ æ’­æ¢¯åº¦ã€‚å¦‚æœæƒ³è¦è®¡ç®—å„ä¸ªVariableçš„æ¢¯åº¦ï¼Œåªéœ€è°ƒç”¨æ ¹èŠ‚ç‚¹variableçš„`backward`æ–¹æ³•ï¼Œautogradä¼šè‡ªåŠ¨æ²¿ç€è®¡ç®—å›¾åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¯ä¸€ä¸ªå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦ã€‚

### æ±‚å¯¼

æŒ‡å®šå‚æ•°å¯ä»¥è¿›è¡Œæ±‚è§£æ¢¯åº¦

```python
#åœ¨åˆ›å»ºtensorçš„æ—¶å€™æŒ‡å®šrequires_grad
a = t.randn(3,4, requires_grad=True)
# æˆ–è€…
a = t.randn(3,4).requires_grad_()
# æˆ–è€…
a = t.randn(3,4)
a.requires_grad=True
a
```

å‡è®¾è¿›è¡Œæ¢¯åº¦æ±‚è§£ $d = sum(c=(a+b))$ 

ä¹Ÿå°±æ˜¯

```python
a = t.randn(3,4, requires_grad=True) # aå…è®¸æ±‚å¯¼
b = t.zeros(3,4).requires_grad_() # bå…è®¸æ±‚å¯¼
c = a+b # æ²¡æœ‰æŒ‡å®šcèƒ½å¤Ÿæ±‚å¯¼
d = c.sum() 
d.backward() #è¿›è¡Œåå‘ä¼ æ’­
```

```python
d # dè¿˜æ˜¯ä¸€ä¸ªrequires_grad=Trueçš„tensor,å¯¹å®ƒçš„æ“ä½œéœ€è¦æ…é‡
d.requires_grad # True
```

```python
a.grad
```

```
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
```

```python
# æ­¤å¤„è™½ç„¶æ²¡æœ‰æŒ‡å®šcéœ€è¦æ±‚å¯¼ï¼Œä½†cä¾èµ–äºaï¼Œè€Œaéœ€è¦æ±‚å¯¼ï¼Œ
# å› æ­¤cçš„requires_gradå±æ€§ä¼šè‡ªåŠ¨è®¾ä¸ºTrue
a.requires_grad, b.requires_grad, c.requires_grad
```

```python
# ç”±ç”¨æˆ·åˆ›å»ºçš„variableå±äºå¶å­èŠ‚ç‚¹ï¼Œå¯¹åº”çš„grad_fnæ˜¯None
a.is_leaf, b.is_leaf, c.is_leaf

# (True, True, False)
```

æ±‚å¯¼ 
$$
y=x^2*e^x
$$
æ±‚å¯¼ç»“æœ
$$
{dy \over dx}=2x *e^x + x^2 * e^x
$$

```python
def f(x):
    '''è®¡ç®—y'''
    y = x**2 * t.exp(x)
    return y

def gradf(x):
    '''æ‰‹åŠ¨æ±‚å¯¼å‡½æ•°'''
    dx = 2*x*t.exp(x) + x**2*t.exp(x)
    return dx
```

```python
x = t.randn(3,4, requires_grad = True)
y = f(x)
y
'''
tensor([[1.7010e-03, 1.7171e+00, 2.9237e-01, 1.1170e-01],
        [1.4809e-03, 2.6344e+01, 1.3164e-01, 4.0763e+00],
        [3.2292e-01, 2.2965e+01, 5.3342e-01, 5.3551e-01]],
       grad_fn=<MulBackward0>)
'''
```

æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦

```python
gradf(x) 
tensor([[-0.0791,  5.7351, -0.4285,  0.8841],
        [-0.0740, 53.4620, -0.4462, 11.2205],
        [-0.4075, 47.4507, -0.0703, -0.0598]], grad_fn=<AddBackward0>)
```

è‡ªåŠ¨æ±‚å¯¼

```python
y.backward(t.ones(y.size())) # gradientå½¢çŠ¶ä¸yä¸€è‡´
x.grad
'''
tensor([[-0.0791,  5.7351, -0.4285,  0.8841],
        [-0.0740, 53.4620, -0.4462, 11.2205],
        [-0.4075, 47.4507, -0.0703, -0.0598]])
'''
```

### è®¡ç®—å›¾

PyTorchä¸­`autograd`çš„åº•å±‚é‡‡ç”¨äº†è®¡ç®—å›¾ï¼Œè®¡ç®—å›¾æ˜¯ä¸€ç§ç‰¹æ®Šçš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œç”¨äºè®°å½•ç®—å­ä¸å˜é‡ä¹‹é—´çš„å…³ç³»ã€‚ä¸€èˆ¬ç”¨çŸ©å½¢è¡¨ç¤ºç®—å­ï¼Œæ¤­åœ†å½¢è¡¨ç¤ºå˜é‡ã€‚å¦‚è¡¨è¾¾å¼$ğ³ = ğ°ğ± + ğ› $å¯åˆ†è§£ä¸º$ğ² = ğ°ğ±$å’Œ$ğ³ = ğ² + ğ›$ï¼Œå…¶è®¡ç®—å›¾å¦‚ä¸‹æ‰€ç¤ºï¼Œå›¾ä¸­`MUL`ï¼Œ`ADD`éƒ½æ˜¯ç®—å­ï¼Œğ°ï¼Œğ±ï¼Œğ›å³å˜é‡ã€‚

![](D:\MarkDown\DeepLearning\img\com_graph.svg)

å¯¹$z=wx+b$è¿›è¡Œæ±‚å¯¼

$${\partial z \over \partial b} = 1,\space {\partial z \over \partial y} = 1\\
{\partial y \over \partial w }= x,{\partial y \over \partial x}= w\\
{\partial z \over \partial x}= {\partial z \over \partial y} {\partial y \over \partial x}=1 * w\\
{\partial z \over \partial w}= {\partial z \over \partial y} {\partial y \over \partial w}=1 * x\\$$



åœ¨åå‘ä¼ æ’­ä¸­éå¶å­èŠ‚ç‚¹çš„å¯¼æ•°è®¡ç®—å®Œæˆåç«‹å³è¢«æ¸…ç©ºå¦‚æœæƒ³è¦æŸ¥çœ‹è¿™äº›å˜é‡å¯ä»¥ä½¿ç”¨ä½¿ç”¨autograd.gradå‡½æ•°æˆ–è€…hook

```python
# ç¬¬ä¸€ç§æ–¹æ³•ï¼šä½¿ç”¨gradè·å–ä¸­é—´å˜é‡çš„æ¢¯åº¦
x = t.ones(3, requires_grad=True)
w = t.rand(3, requires_grad=True)
y = x * w
z = y.sum()
# zå¯¹yçš„æ¢¯åº¦ï¼Œéšå¼è°ƒç”¨backward()
print(t.autograd.grad(z, y))
print(t.autograd.grad(z,x))
# print(t.autograd.grad(z,w))
```

```python
# ç¬¬äºŒç§æ–¹æ³•ï¼šä½¿ç”¨hook
# hookæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥æ˜¯æ¢¯åº¦ï¼Œä¸åº”è¯¥æœ‰è¿”å›å€¼
def variable_hook(grad):
    print('yçš„æ¢¯åº¦ï¼š',grad)

x = t.ones(3, requires_grad=True)
w = t.rand(3, requires_grad=True)
y = x * w
# æ³¨å†Œhook
hook_handle = y.register_hook(variable_hook)
z = y.sum()
z.backward()

# é™¤éä½ æ¯æ¬¡éƒ½è¦ç”¨hookï¼Œå¦åˆ™ç”¨å®Œä¹‹åè®°å¾—ç§»é™¤hook
hook_handle.remove()
```

### æ‰©å±•autograd



è‡ªå®šä¹‰autogradå®ç°åå‘æ±‚å¯¼

```python
class Mul(Function):

    @staticmethod
    def forward(ctx, w, x, b, x_requires_grad = True):
        ctx.x_requires_grad = x_requires_grad
        ctx.save_for_backward(w,x)
        output = w * x + b
        return output

    @staticmethod
    def backward(ctx, grad_output):
        w,x = ctx.saved_tensors
        grad_w = grad_output * x
        if ctx.x_requires_grad:
            grad_x = grad_output * w
        else:
            grad_x = None
        grad_b = grad_output * 1
        return grad_w, grad_x, grad_b, None
```



è‡ªå®šä¹‰çš„Functionéœ€è¦ç»§æ‰¿autograd.Functionï¼Œæ²¡æœ‰æ„é€ å‡½æ•°`__init__`ï¼Œforwardå’Œbackwardå‡½æ•°éƒ½æ˜¯é™æ€æ–¹æ³•

backwardå‡½æ•°çš„è¾“å‡ºå’Œforwardå‡½æ•°çš„è¾“å…¥ä¸€ä¸€å¯¹åº”ï¼Œbackwardå‡½æ•°çš„è¾“å…¥å’Œforwardå‡½æ•°çš„è¾“å‡ºä¸€ä¸€å¯¹åº”

backwardå‡½æ•°çš„grad_outputå‚æ•°å³t.autograd.backwardä¸­çš„`grad_variables`

å¦‚æœæŸä¸€ä¸ªè¾“å…¥ä¸éœ€è¦æ±‚å¯¼ï¼Œç›´æ¥è¿”å›Noneï¼Œå¦‚forwardä¸­çš„è¾“å…¥å‚æ•°x_requires_gradæ˜¾ç„¶æ— æ³•å¯¹å®ƒæ±‚å¯¼ï¼Œç›´æ¥è¿”å›Noneå³å¯

åå‘ä¼ æ’­å¯èƒ½éœ€è¦åˆ©ç”¨å‰å‘ä¼ æ’­çš„æŸäº›ä¸­é—´ç»“æœï¼Œéœ€è¦è¿›è¡Œä¿å­˜ï¼Œå¦åˆ™å‰å‘ä¼ æ’­ç»“æŸåè¿™äº›å¯¹è±¡å³è¢«é‡Šæ”¾