本周阅读论文《Context Prior for Scene Segmentation》以及论文《Strip Pooling: Rethinking Spatial Pooling for Scene Parsing》论文相关代码的理解。

#### 《Context Prior for Scene Segmentation》

#### 摘要



为了获得更精确的语义分割结果，最近的研究已经开始广泛的探索上下文依赖关系。但是很少区分不同类型的上下文依赖，这可能会影响上下文的理解。作者在这项工作中使用监督特征进行聚合，区分类内上下文和类间上下文。具体来说就是使用亲和力损失的监督下开发了上下文的先验。给定一幅输入图像和相应的背景信息，亲和力损失算法构建一个理想的亲和力映射来监督上下文先验的学习。学习上下文先验提取属于同一类别的像素，而反转的上下文先验则聚焦于不同类别的像素。将上下文优先层嵌入到传统的深度CNN中，可以选择性地捕捉类内和类间的上下文依赖关系，从而获得稳健的特征表示。为了验证算法的有效性，作者设计了一个有效的上下文优先网络(CPNet)。大量的定量和定性评估表明，该模型与最新的语义切分方法相比具有良好的性能。更具体地说，作者的算法在ADE20K上达到了46.3%的mIoU，在PASCAL上下文上达到了53.9%，在Cityspace数据集上达到了81.3%。



#### 1、介绍

场景分割是计算机视觉中一个长期存在且极具挑战性的问题，有许多下游应用，如增强现实、自动驾驶、人机交互和视频内容分析。目标是为每个像素指定一个类别标签，以提供全面的场景理解。

得益于全卷积网络(FCN)的有效特征表示，一些方法已经取得了良好的性能。然而，由于受卷积层结构的限制，FCN提供的上下文信息不够充分，留下了改进的空间。因此，各种方法探索上下文依赖关系以获得更准确的分割结果。上下文信息的聚合主要有两种途径：1)基于金字塔的聚合方法。此方法捕获了类间的上下文关系但是不同类间的关系没有捕获到。2)基于注意力的聚合方法。最近的基于注意力的方法学习通道注意、空间注意或逐点注意来选择性地聚集异构上下文信息。然而，由于缺乏明确的规律性，注意机制的关系描述不够清晰。因此，它可能会选择不需要的上下文依赖关系。总体而言，这两条路径在没有明确区分的情况下聚合上下文信息，导致不同上下文关系的混合。

![image-20200802102412200](D:\MarkDown\DeepLearning\img\image-20200802102412200.png)

作者构造了上下文先验来建模类内和类间依赖作为先验知识。将上下文先验表示为二进制分类器，以区分哪些像素属于当前像素的同一类别，而反转的先验可以聚焦于不同类别的像素。具体地说就是首先使用完全卷积网络来生成特征映射和对应的先验映射。对于特征映射中的每个像素，先验映射可以选择性地突出显示属于相同类别的其他像素以聚合类内上下文，而反转的先验可以聚合类间上下文。为了将先验嵌入到网络中，作者开发了一个包含亲和力损失的上下文优先层，它直接监督先验的学习。同时，语境先验也需要空间信息来推理关系。

为了验证上下文先验算法的有效性，作者设计了一个简单的全卷积网络，称为上下文先验网络(CPNet)。基于骨干网的输出特征，上下文优先层使用聚合模块聚合空间信息以生成上下文优先图。在亲和力损失的监督下，上下文先验映射能够捕获类内上下文和类间上下文，从而提高预测精度。大量的评估结果表明，该方法与最近几种最新的语义切分方法相比具有较好的性能。



论文的主要贡献：

1、通过在上下文优先层中嵌入亲和力损失的监督，构造上下文先验，以显式地捕获类内和类间上下文依赖

2、设计了一种有效的上下文优先网络(CPNet)来进行场景分割，它包括一个骨干网络和一个上下文优先层。

3、在ADE20K、PASCAL-CONTEXT和Cityscape的基准测试上证明了所提出的方法比最先进的方法具有更好的性能。更具体地说，作者的单一模型在ADE20K验证集上达到了46.3%，在PASCALContext验证集上达到了53.9%，在CITYSCAPES测试集上达到了81.3%。



####  Context Prior



作者提出了一个上下文先验(Context Prior)来建模同一类别的像素(上下文内)和不同类别的像素(上下文间)之间的关系。在上下文先验的基础上，作者提出了上下文先验网络，将上下文先验层与亲和度损失的监督结合在一起，如图2所示。在本节中，作者首先介绍亲和度损失，它监督该层学习上下文先验映射。接下来，作者展示了Context Prior Layer，它学习了Context Prior Map来将上下文内和上下文间聚合到一个像素上。聚集模块用于聚集用于推理的空间信息。最后，详细阐述了完整的网络结构。

![image-20200802155932107](D:\MarkDown\DeepLearning\img\image-20200802155932107.png)

##### 亲和度损失

在场景分割任务中，对于每个图像，有一个基本事实，它为每个像素分配一个语义类别。网络很难对孤立像素的上下文信息进行建模。为了显式地规则化网络以建模类别之间的关系，作者引入了亲和力损失。对于图像中的每个像素，这种损失迫使网络考虑相同类别的像素(上下文内)和不同类别之间的像素(上下文间)。

给定输入的基本事实，作者可以知道每个像素的“上下文先验”(即，哪些像素属于同一类别，哪些像素不属于)。可以在根据地面事实指导网络之前了解上下文。为此，作者首先构建了一个以地面真实为监督的理想亲和力地图。给定输入图像I和地面实况L，作者将输入图像I馈送到网络，获得大小为H×W的特征地图X。如图3所示，作者首先将地面实况L向下采样到与特征地图X相同的大小，从而产生较小的地面实况L。

![image-20200802160359504](D:\MarkDown\DeepLearning\img\image-20200802160359504.png)

作者使用二分类交叉损失函数

![image-20200802160501653](D:\MarkDown\DeepLearning\img\image-20200802160501653.png)

然而，这种一元损失只考虑了先前映射中的孤立像素，而忽略了与其他像素的语义相关性。先验地图P的每一行像素与特征地图X的像素相对应，作者可以将它们分为类内像素和类间像素，它们之间的关系有助于推理语义相关性和场景结构。因此，作者可以将类内像素和类间像素作为两个整体分别进行编码。

因此得到如下的损失函数：

![image-20200802160600432](D:\MarkDown\DeepLearning\img\image-20200802160600432.png)

总的损失函数被表示为:

![image-20200802160639596](D:\MarkDown\DeepLearning\img\image-20200802160639596.png)

##### 聚合模型

作者设计了一个高效的聚合模块，该模块采用完全可分离的卷积(在空间和深度维度上都是分离的)来聚合空间信息。卷积层可以固有地聚集附近的空间信息。聚集更多空间信息的一种自然方法是使用较大的滤波器大小卷积。然而，具有大滤波器大小的卷积在计算上是昂贵的。作者在空间上将标准卷积分解为两个非对称卷积。对于k×k卷积，作者可以使用k×1卷积，然后是1×k卷积作为另一种选择，称为空间可分离卷积。与标准卷积相比，它可以减少2次运算量，并保持接收场的大小不变。同时，每个空间可分离卷积都采用深度卷积，进一步降低了计算量。作者称这种可分离卷积为完全可分离卷积，同时考虑了空间和深度两个维度。图4演示了聚合模块的完整结构。

![image-20200802160840851](D:\MarkDown\DeepLearning\img\image-20200802160840851.png)

##### 网络结构

上下文优先网络(CPNet)是由骨干网和上下文优先层组成的全卷积网络，如图2所示，骨干网是现成的卷积网络，例如ResNet，具有扩张策略。在上下文优先层，聚集模块首先高效地聚集一些空间信息。基于聚集的空间信息，上下文优先层学习上下文先验映射以捕获类内上下文和类间上下文。同时，亲和力损失使上下文先验学习正规化，而交叉熵损失函数则是分割监督。在开创性工作的基础上，作者在骨干网络的第四阶段采用了辅助损耗，这也是一种交叉熵损耗。损失函数如下：

![image-20200802161052891](D:\MarkDown\DeepLearning\img\image-20200802161052891.png)



#### 实验结果

作者测试了宝库ADE20K，PASCAL-Context和Cityspace数据集并利用Pytorch实现了自己的网络。

##### 实现细节



**网络**：作者采用ResNet作为带扩张策略的预训练模型。然后采用双线性插值对预测进行8次上采样，计算分割损失。之后，作者对主干网第四阶段的辅助损耗进行积分。作者将聚合模块中完全可分离卷积的滤波器大小设置为11。



**数据参数：**

在训练阶段，作者对输入图像采用均值减法、随机水平翻转和随机尺度，其中包含{0.5，0.75，1.0，1.5，1.75，2.0}，以避免过拟合。最后，作者随机裁剪大图像或将小图像填充成固定大小用于训练(ADE20K为480×480，Pascal上下文为512×512，城市景观为768×768)。



**优化:** 

作者使用随机梯度下降算法微调了CPnet模型，动量为0.9，重量衰减为1e-4，批次为16。当在城市景观数据集上进行训练时，作者将权重衰减设置为5×10−4。



**推理阶段**

在推理阶段，作者对多个缩放和翻转输入的预测进行平均，以进一步提高性能。对于ADE20K和Pascal上下文数据集，作者使用包括{0.5，0.75，1.0，1.5，1.75}在内的比例，而对于城市景观数据集，作者使用{0.5，0.75，1，1.5}。此外，作者还采用像素精确度(PixAcc)和并的平均交集(Miou)作为评价指标。



#### 在ADE20K数据集上进行验证



数据集描述：由于其复杂的场景和多达150个类别标签，ADE20K是一个具有挑战性的场景解析基准。该数据集分为20k/2k/3k分别用于培训、验证和测试。作者使用picAcc和mIoU在验证集上报告结果。

![image-20200802162850517](D:\MarkDown\DeepLearning\img\image-20200802162850517.png)

为了验证Context Prior和CPNet的有效性，作者在不同的设置下进行了实验，并与其他空间信息聚合模块进行了比较，如上图所示。

基于FCN提取的特征，各种方法聚集上下文信息以提高性能。基于金字塔的方法(如PSP和ASPP)采用金字塔汇集或金字塔扩张率来聚合多范围空间信息。应用自我注意方法来聚集远程空间信息，而PSA模块学习过参数的逐点注意。上图列出了作者使用不同空间信息聚合模块的重新实现结果。虽然这些方法可以在基线上提高性能，但它们将空间信息聚合为类内上下文和类间上下文的混合，可能会使网络混乱，如第1节所讨论的。因此，与这些方法不同的是，所提出的CPNet在对识别的上下文关系进行编码之前将上下文依赖视为上下文。具体地说，对于每个像素，作者使用Context  Prior层捕获类内上下文和类间上下文。在相同的主干ResNet-50和没有其他测试技巧的情况下，作者的方法比这些方法具有更好的性能。

ADE20K验证集的可视化改进。获取类内上下文和类间上下文有助于场景理解。对比如下图所示：

![image-20200802163308738](D:\MarkDown\DeepLearning\img\image-20200802163308738.png)



对ADE20K验证集进行定量评估。与最先进的分割算法相比，所提出的CPNet具有良好的性能。

![image-20200802163420892](D:\MarkDown\DeepLearning\img\image-20200802163420892.png)







#### 论文《Strip Pooling: Rethinking Spatial Pooling for Scene Parsing》相关代码

该论文的主要是使用了 Strip pooling  

```python
class Identity(nn.Module):
    def __init__(self):
        super(Identity, self).__init__()

    def forward(self, x):
        return x

class GlobalPooling(nn.Module):
    def __init__(self, in_channels, out_channels, norm_layer, up_kwargs):
        super(GlobalPooling, self).__init__()
        self._up_kwargs = up_kwargs
        self.gap = nn.Sequential(nn.AdaptiveAvgPool2d(1),
                                 nn.Conv2d(in_channels, out_channels, 1, bias=False),
                                 norm_layer(out_channels),
                                 nn.ReLU(True))

    def forward(self, x):
        _, _, h, w = x.size()
        pool = self.gap(x)
        return interpolate(pool, (h,w), **self._up_kwargs)
```





SPNet网络实现

```python
class SPNet(BaseNet):
    def __init__(self, nclass, backbone, pretrained, criterion=None, aux=True, norm_layer=nn.BatchNorm2d, spm_on=False, **kwargs):
        super(SPNet, self).__init__(nclass, backbone, aux, pretrained, norm_layer=norm_layer, spm_on=spm_on, **kwargs)
        self.head = SPHead(2048, nclass, norm_layer, self._up_kwargs)
        self.criterion = criterion
        if aux:
            self.auxlayer = FCNHead(1024, nclass, norm_layer)

    def forward(self, x, y=None):
        _, _, h, w = x.size()
        _, _, c3, c4 = self.base_forward(x)

        x = self.head(c4)
        x = interpolate(x, (h,w), **self._up_kwargs)
        if self.aux:
            auxout = self.auxlayer(c3)
            auxout = interpolate(auxout, (h,w), **self._up_kwargs)
        
        if self.training:
            aux = self.auxlayer(c3)
            aux = interpolate(aux, (h, w), **self._up_kwargs)
            main_loss = self.criterion(x, y)
            aux_loss = self.criterion(aux, y)
            return x.max(1)[1], main_loss, aux_loss
        else:
            return x
```

