本周阅读论文《Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision》以及上周代码的理解。



## 论文部分

### 摘要

基于卷积神经网络的方法在语义分割方面取得了显着的进展。然而，这些方法严重依赖于劳动密集型的注释数据。为了应对这一限制，使用从图形引擎生成的自动标注数据来训练分割模型。然而，从合成数据训练的模型很难转换成真实的图像。为了解决这个问题，以前的工作已经考虑将模型从源数据直接适配到未标记的目标数据(以缩小域间差距)。然而，这些技术没有考虑目标数据本身之间的巨大分布差距(域内差距)。在这项工作中，作者提出了一种两步自监督的域自适应方法，以最小化域间和域内的差距。首先，作者进行了模型的域间适配，通过这种适配，作者使用基于熵的排序函数将目标领域划分为容易的和硬的两个分裂。最后，为了缩小域内差距，作者提出了一种从容易分裂到硬分。裂的自监督自适应技术。在大量基准数据集上验证了作者方法的有效性和先进性。



如下图所示,作者提出了一种两步自监督的领域自适应语义分割技术。以往的工作只是将分割模型从源域调整到目标域。作者的工作还考虑在目标域内从干净的映射到有噪声的映射。

![image-20200823163837503](D:\MarkDown\DeepLearning\img\image-20200823163837503.png)





### 介绍

语义分割的目的是将图像中的每个像素分配给一个语义类。近年来，基于卷积神经网络的分割模型[取得了显著的进展，在自动驾驶、机器人学和疾病诊断等计算机视觉系统中得到了广泛的应用。训练这样的分段网络需要大量的注释数据。

然而，收集包含像素级注释的大型数据集用于语义分割是困难的，因为它们昂贵且耗费人力。最近，从模拟器和游戏引擎呈现的具有精确像素级语义注释的照片真实感数据已被用于训练分割网络。然而，由于跨域的差异，从合成数据训练的模型很难转移到真实数据。为了解决这一问题，提出了无监督域自适应(UDA)技术来对齐已标记的源数据和未标记的目标数据之间的分布偏移。对于特定的语义分割任务，基于对抗性学习的UDA方法显示出在图像或输出级别上对齐特征的有效性。虽然许多模型考虑了单源-单目标的适应设置，但最近的工作提出了解决多源域的问题，它侧重于多源-单目标的适应设置。最重要的是，以前的工作大多考虑将模型从源数据调整到目标数据(域间差距)。

然而，从现实世界中收集到的目标数据具有不同的场景分布，这些分布是由移动物体、天气条件等各种因素造成的，从而导致目标之间存在较大的差距(域内差距)。例如，图1所示的目标域中的噪声图和干净图是由同一模型对不同图像做出的预测。虽然以往的研究仅仅着眼于缩小域间差距，但域内差距问题却相对较少引起人们的关注。在本文中，作者提出了一种两步域自适应的方法来最小化域间和域内的差距。作者的模型由三部分组成，如图2所示，即：1)域间适配模块，用于弥合已标记的源数据和未标记的目标数据之间的域间差距；2)基于熵的排序系统，用于将目标数据分成容易拆分和硬拆分；以及3)域内适配模块，用于弥合容易拆分和硬拆分之间的域内差距(使用来自容易子域的伪标签)。在语义分割方面，作者提出的方法在基准数据集上取得了比最先进的方法更好的性能。此外，作者的方法比以前的数字分类领域自适应方法有更好的性能。

![image-20200823164200098](D:\MarkDown\DeepLearning\img\image-20200823164200098.png)



### 取得的成就

首先，作者引入了目标数据之间的域间差距，并提出了一种基于熵的排序函数，将目标域分成容易子域和硬子域。其次，作者提出了一种两步自监督的域自适应方法，以最小化域间和域内的差距。



### Approach

#### 域间适应

样本$X_s\in R^{H\times W\times3}$来自源域及其关联的映射$Y_s$没和十日$Y_s^{(h,w)}=[Y_s^{(h,w,c)}]$的像素$(h,w)$作为一个one-hot向量进行输入。具体公式如下:

![image-20200823165101823](C:\Users\liao\AppData\Roaming\Typora\typora-user-images\image-20200823165101823.png)

为了弥合源域和目标域之间的域间差距，建议利用熵图来对齐特征的分布偏移。训练的模型倾向于对类似源的图像产生过度自信(低熵)的预测，而对类似目标的图像产生不太自信(高熵)的预测。生成器$G_{intert}$将目标图像$X_t$作为输入，并产生分割图$P_t=G_{inter}(X_t)$。

![image-20200823165437365](D:\MarkDown\DeepLearning\img\image-20200823165437365.png)

根据1和2式得到：

![image-20200823165555393](D:\MarkDown\DeepLearning\img\image-20200823165555393.png)

#### 基于熵的排序

由于不同的天气条件、移动对象和阴影，从真实世界收集的目标图像具有不同的分布。在图2中，一些目标预测图是干净的，而另一些则非常嘈杂，尽管它们是从相同的模型生成的。由于目标图像之间存在域内间隙，一种简单的解决方案是将目标域分解为较小的子域/拆分。然而，由于缺乏靶标，这仍然是一项具有挑战性的任务。为了建立这些分裂，作者利用熵图来确定目标预测的置信度。生成器$G_{intert}$接受目标图像$X_t$作为输入以生成$P_t$并对其进行熵映射。在此基础上，作者采用了一种简单而有效的排名方式，使用金星和排名：

![image-20200823165746283](D:\MarkDown\DeepLearning\img\image-20200823165746283.png)



### 实验

在GTA5(A)、Synthia(B)和Synscape(C)上训练的模型集对城市景观的语义分割结果进行了验证。所有结果都是由基于ResNet-101的模型生成的。在(A)和(B)的实验中，使用Coment作为域间适应和域内适应的框架。在(C)的实验中，使用AdaptSegNet作为域间自适应和域内自适应的框架。mIoU在(B)中表示13个类别的平均值，不包括带*的类别。

![image-20200823170157688](D:\MarkDown\DeepLearning\img\image-20200823170157688.png)



超参数λ不同取值获得的结果

![image-20200823170436796](D:\MarkDown\DeepLearning\img\image-20200823170436796.png)

见GTA5数据集迁移到Cityscape数据集上的表现

![image-20200823170528073](D:\MarkDown\DeepLearning\img\image-20200823170528073.png)

实验结果可视化

将GTA5数据集迁移到Cityscape数据集上的预测结果。。(A)和(D)是来自城市景观验证集的图像和相应的地面真实注释。(B)是域间适应的预测分割图。(C)是根据作者的技术预测的地图。

![image-20200823170806232](D:\MarkDown\DeepLearning\img\image-20200823170806232.png)



## 代码部分

上周的论文主要是一种自我监督的等变注意机制(SEAM)来发现额外的监督并缩小差距。作者的方法是基于等差是全监督语义分割中的一个隐含约束，其像素级标签在数据增强过程中采取与输入图像相同的空间变换。

SEAM网络的主干代码如下：

```python
class Net(network.resnet38d.Net):
    def __init__(self):
        super(Net, self).__init__()
        self.dropout7 = torch.nn.Dropout2d(0.5)

        self.fc8 = nn.Conv2d(4096, 21, 1, bias=False)

        self.f8_3 = torch.nn.Conv2d(512, 64, 1, bias=False)
        self.f8_4 = torch.nn.Conv2d(1024, 128, 1, bias=False)
        self.f9 = torch.nn.Conv2d(192+3, 192, 1, bias=False)
        
        torch.nn.init.xavier_uniform_(self.fc8.weight)
        torch.nn.init.kaiming_normal_(self.f8_3.weight)
        torch.nn.init.kaiming_normal_(self.f8_4.weight)
        torch.nn.init.xavier_uniform_(self.f9.weight, gain=4)
        self.from_scratch_layers = [self.f8_3, self.f8_4, self.f9, self.fc8]
        self.not_training = [self.conv1a, self.b2, self.b2_1, self.b2_2]
....
```

这里使用的是restnet32作为主干网络进行设计。



损失函数

```python
def adaptive_min_pooling_loss(x):
    n,c,h,w = x.size()
    k = h*w//4
    x = torch.max(x, dim=1)[0]
    y = torch.topk(x.view(n,-1), k=k, dim=-1, largest=False)[0]
    y = F.relu(y, inplace=False)
    loss = torch.sum(y)/(k*n)
    return loss
```

进行训练代码

```python
 args = parser.parse_args()
    crf_alpha = [4,24]
    model = getattr(importlib.import_module(args.network), 'Net')()
    model.load_state_dict(torch.load(args.weights))

    model.eval()
    model.cuda()
        
    infer_dataset = voc12.data.VOC12ClsDatasetMSF(args.infer_list, voc12_root=args.voc12_root,
                                                  scales=[0.5, 1.0, 1.5, 2.0],
                                                  inter_transform=torchvision.transforms.Compose(
                                                       [np.asarray,
                                                        model.normalize,
                                                        imutils.HWC_to_CHW]))

    infer_data_loader = DataLoader(infer_dataset, shuffle=False, num_workers=args.num_workers, pin_memory=True)

    n_gpus = torch.cuda.device_count()
    model_replicas = torch.nn.parallel.replicate(model, list(range(n_gpus)))
```

