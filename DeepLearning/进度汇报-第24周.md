本周阅读论文《Temporally Distributed Networks for Fast Video Semantic Segmentation》。本片论文主要是针对视频具有时间上连续性这一特点进行研究并取得了最高水平的准确率。



### 摘要

作者提出了一个专为快速准确的视频语义分割而设计的时间分布网络TDNet。根据观察从深层CNN的某一高层提取的特征可以通过组合从几个较浅的子网提取的特征来近似。利用视频固有的时间连续性将这些子网分布在连续的帧上。因此，在每个时间步只需要进行一次轻量级计算，就可以从单个子网中提取一个子特征组。然后，通过应用一种新的注意力传播模块来重新组合用于分割的全部特征，该模块补偿帧之间的几何变形。同时引入分组知识提炼损失，进一步提高了全特征和子特征两个层次的表示能力。在CitySces、CamVid和NYUD-v2上的实验表明，该方法在速度和延迟上都达到了最高水平的准确率。



### 介绍

视频语义分割的目的是为视频帧分配像素级的语义标签。作为视觉理解的一项重要任务，它越来越受到研究界的关注。最近在密集标注任务中取得的成功表明，强特征表示对于准确的分割结果至关重要。然而，计算强大的功能通常需要深入的网络，计算成本很高，这使得它对现实世界的应用(如自动驾驶汽车、机器人传感和增强现实)具有挑战性，这些应用既需要高精度，也需要低延迟。

视频语义分割最直接的策略是对每一帧单独应用深度图像分割模型，但该策略没有利用视频动态场景中提供的时间信息。

一种解决方案是将相同的模型应用于所有帧，并在顶部添加附加层以建模时间上下文，以提取更好的特征。然而，这样的方法并不能帮助提高效率，因为必须在每一帧重新计算所有特征。为了减少冗余计算，一个合理的方法是只在关键帧应用强图像分割模型，并将高级特征重用于其他帧。然而，其他帧相对于关键帧的空间未对准很难补偿，并且经常导致精度降低。此外，这些方法在关键帧和非关键帧之间具有不同的计算量，导致较高的最大延迟和计算资源占用的不平衡，从而可能降低系统效率。



为了应对这些挑战，作者提出了一种新的深度学习模型，用于高精度、低延迟的语义视频分割，称为时间分布网络(TDNet)。模型受分组卷积的启发，它表明用分离的滤波器组提取特征不仅允许模型并行化，而且有助于学习更好的表示。



给定一个像PSPNet这样的深度图像分割网络，将深度模型提取的特征分成N个(例如N=2或4)组，并使用N个不同的浅子网络来近似每组特征通道。通过强制每个子网络覆盖单独的特征子空间，可以通过重组这些子网络的输出来产生强特征表示。为了随着时间的推移实现平衡和高效的计算，让N个子网络共享相同的浅层体系结构，将其设置为原始深层模型大小的1N，以保持类似的总模型容量。

![image-20200809141243301](D:\MarkDown\DeepLearning\img\image-20200809141243301.png)



为了进一步增强网络的表征能力，提出了分组蒸馏损失，以将知识从全深度模型传递到作者的分布式特征网络中，在全特征组和子特征组级上都是如此。在这种新模型下，作者只需要在每一帧上进行一次轻量级的前向传播，并且可以通过重用前一帧中提取的子特征来聚合完整的特征。如图所示，作者的方法在保持较低延迟的同时优于最先进的方法。作者通过在多个基准上的大量实验来验证作者的方法。



#### 论文贡献



1、时间分布的网络体系结构和分组的知识蒸馏损失，它以相当的精度加速了最先进的视频语义分割模型，延迟超过2倍；

2、注意力传播模块可以随着时间的推移有效地聚合分布式特征组，对帧之间的几何变化具有鲁棒性；

3、在Citycapes、Camvid和NYUD-v2这三个具有挑战性的数据集上，与以前的视频语义分割方法相比，具有更高的精确度和延迟。



### Temporally Distributed Network（TDNet）



如下图网络结构所示。与使用单个深度模型独立分割每一帧不同，在TDNet(B)中，作者将特征提取均匀地分布在连续的帧上以减少冗余计算，然后使用注意力传播模块(APM)对它们进行聚合，以获得强特征以实现准确的分割。

![image-20200809141735560](D:\MarkDown\DeepLearning\img\image-20200809141735560.png)



采用单独的卷积路径可以通过增强过滤关系的稀疏性来提高模型的有效性。受此启发，作者建议将深层神经网络中的特征划分为一组子特征，并使用一组浅子网络来逼近它们，每个浅子网络仅覆盖原始模型特征表示的一个子空间。



此外，作者观察到完整的特征图很大，并且降维(图2(A))花费的计算代价时昂贵的。PSPNet50中，特征映射有4096个通道，降维约占总计算量的三分之一。为了进一步提高效率，基于分块矩阵乘法，作者将用于降维的卷积层转换为子空间级别的卷积运算序列的求和，这使得作者能够将这些子空间级别的卷积运算分配到各自的子网。结果，降维层的输出在用于网络的预测头部之前简单地通过加法进行重组。在保持与原始深层模型相似的总模型大小的情况下，作者证明了聚合多条浅层网络路径可以具有与原始深层模型类似的强大表征能力。



#### Feature Aggregation

聚合在不同时间步长提取的特征组的一大挑战是帧之间的运动引起的空间不对齐。目前流行的方法时Optical flow-based warping，但其计算成本高，容易出错，并且限于每个像素一次匹配。为了应对这样的挑战，作者提出了一种注意力传播模块(APM)，该模块基于非局部注意力机制，但被扩展以处理视频语义分割任务的时空变化。



![image-20200809142312882](D:\MarkDown\DeepLearning\img\image-20200809142312882.png)



如上图所示，TDNet由两个阶段组成，编码阶段和分段阶段。编码阶段提取随时间变化的交替子特征映射。除了生成包含路径特定子特征组的V值特征映射之外，作者还让子网络生成查询和关键映射，用于在帧之间建立像素之间的相关性。

在分割阶段，目标是根据前一帧中子网络的输出重新组合出完整的特征来产生分割结果。假设具有从视频帧导出的m个(上图中的m=4)个独立的特征路径，并且想要通过将前m-1个帧的输出与当前帧相结合来构建帧t的全特征表示。作者通过时空关注来实现这一点，其中作者独立地计算当前帧t和先前m-1帧的像素之间的亲和度。

![image-20200809142851819](D:\MarkDown\DeepLearning\img\image-20200809142851819.png)

其中p表示前一帧，$d_k$是查询和关键字的维度。然后，将当前帧和前m-1帧的子特征地图合并为，

![image-20200809143004127](D:\MarkDown\DeepLearning\img\image-20200809143004127.png)



#### 注意力下采样

作者采用了一种简单而有效的策略，即对参考数据进行下采样，如图3中的“下采样”模块所示。形式上，当分割帧T时，作者将步长为n的空间池操作γn(·)应用于前m-1帧的查询、关键字和V值映射，

![image-20200809143204199](D:\MarkDown\DeepLearning\img\image-20200809143204199.png)

有了下采样，可以将等式2的复杂度降低到
$$
O(\frac{(m-1)d_kH^2W^2}{n^2})
$$
作者进行了实验，发现n=4能够很好地保存必要的空间信息，同时大大降低了计算成本。

#### Attention Propagation

接下来，作者提出了一种传播方法，不是计算当前帧和之前所有帧之间的关注度，而是将计算限制在相邻的帧上，并通过窗口进行传播。这不仅可以减少必须计算的注意力图的数量，而且还可以将注意力计算限制在运动较小的后续帧。计算公式如下：
![image-20200809143625850](D:\MarkDown\DeepLearning\img\image-20200809143625850.png)



![image-20200809143740877](D:\MarkDown\DeepLearning\img\image-20200809143740877.png)

如上图所示：在“Overall KD”中，我们将教师模型(例如PSPNet101)和学生模型(例如Out  TDNet)之间的全部输出对齐。在“Grouped KD”中，我们将仅基于一个子网络的输出与教师模型的输出进行匹配，该输出以各自的特征子空间为条件。



### 实验

作者在Cityscape、Camvid  for street views和NYUDv2数据集上进行了评估。在所有这些数据集上，我们的方法以更快的速度和更低且均匀分布的延迟实现了最先进的准确率。



#### Cityscape数据集结果

![image-20200809144158034](D:\MarkDown\DeepLearning\img\image-20200809144158034.png)

从上表可以看出再使用了TD-PSP18后速度和Max Latency相较于其他有了较大的进度再TD-PSP50上mIoU有了较大的进步。



#### 其他数据集

![image-20200809144600731](D:\MarkDown\DeepLearning\img\image-20200809144600731.png)

![image-20200809144511328](D:\MarkDown\DeepLearning\img\image-20200809144511328.png)





#### 结果分析

##### 1、训练可视化

![image-20200809144803005](D:\MarkDown\DeepLearning\img\image-20200809144803005.png)

作者提出的方法在城市景观和NYUD-v2(A)上的定性结果，以及在我们的注意力传播网络中的注意力地图的可视化(B)。给定帧t中的一个像素(表示为绿色十字)，我们使用亲和度矩阵反向传播相关分数，然后将归一化软权重可视化为窗口中其他帧上的热图。

##### 2、使用不同方法的效果

TD4-PSP18在城市景观数据集上的不同时间聚集方法。“APM”表示我们的注意力传播模块。“STA”代表时空注意力。“OFW”是基于光流的融合。“添加”就是简单地添加功能地图。

![image-20200809144916877](D:\MarkDown\DeepLearning\img\image-20200809144916877.png)

