本周主要理解了“动态路由”网络的实现，以及相关代码的理解，因为作者使用的是自己实现的框架而非Pytorch或tensorflow等框架。并通过查阅相关资料理解了作者提出的可卷积分离的相关知识。





### 网络模型



#### 整体模型

![image-20200709163144891](D:\MarkDown\DeepLearning\img\image-20200709163144891.png)

如上图所示，该网络具有L层，最大下采样为32。为了稳定，开始的STEM和最终的上采样是固定的。虚线表示动态路由可选的路径。图片右边：cell级别的动态路由处理. 给定来自前一层的输入总和，我们首先使用软控门$Soft$   $Conditional$  $Gate$生成激活权重。相应权重大于零的路径被标记为激活，这将被选择用于特征转换。

从上图可以看出使用最开始使用了STEM块，根据论文的介绍，将分辨率降低到了1/4的分辨率。获得STEM的输出后，以此为基础构建路由空间，如上图左半部分所示。在路由空间中，相邻的Cell之间的比例因子被限制为2，这在基于RestNet的方法中被广泛采用。因此，最小比例设置为1/32。在这些约束条件下，每层候选的数目最多为4个，每个候选图像有3条尺度变换路径，即上采样、保持分辨率和下采样。在每个候选者内部基本单元被设计用于特征聚合，基本门被用于路径选择。

网络配置如下所示：

```python
BACKBONE=dict(
            CELL_TYPE=['sep_conv_3x3', 'skip_connect'],
            LAYER_NUM=16,
            CELL_NUM_LIST=[2, 3, 4] + [4 for _ in range(13)],
            INIT_CHANNEL=64,
            MAX_STRIDE=32,
            SEPT_STEM=True,
            NORM="nnSyncBN",
            DROP_PROB=0.0,
        ),
        GATE=dict(
            GATE_ON=True,
            GATE_INIT_BIAS=1.5,
            SMALL_GATE=False,
        ),

SEM_SEG_HEAD=dict(
   IN_FEATURES=['layer_0', 'layer_1', 'layer_2', 'layer_3'],
   NUM_CLASSES=19,
   IGNORE_VALUE=255,
   NORM="nnSyncBN",
   LOSS_WEIGHT=1.0,
 ),
```

#### 路由处理与软控门

首先聚合来自$l-1$层具有不同空间大小的三个输入(也就是 $s/2、s、2s$) 分别表示为$Y_{s/2}^{l-1}、Y_{s}^{l-1}、Y_{2s}^{l-1}$因此,可以将第l层的输入$X_s^l$表示为
$$
X_s^l=Y_{s/2}^{l-1}+Y_{s}^{l-1}+Y_{2s}^{l-1}
$$
然后聚合的输入将用于$Cell$ $Gate$的内部转换.

每一层数据在进行聚合后的数据使用"separate convolutions"（可分离卷积）和"identity mapping "（恒等映射），这两个概念将在之后进行单独介绍。

通过路由处理这里对每个cell采用基本的特征融合. 然后会生成特征图$H_s^{l}$将根据激活因子$\alpha_s^l$变换到不同的尺度。
$$
H_s^l=\sum_{O^i\in{o}}O^i(X_s^l)
$$
![image-20200703171059084](D:\MarkDown\DeepLearning\img\image-20200703171059084.png)

通过门控函数，在论文中将$\beta_s^l$设置为常量1.5。代码配置如下:

```python
    GATE=dict(
            GATE_ON=True,
            GATE_INIT_BIAS=1.5,
            SMALL_GATE=False,
    )
```

对于网络，如上图的网络结构所示，论文设置了l=16与33两种深度与Restnet50和Restnet101相同。在最后设计了一个简单的解码器将不同的预测输出。具体来说就是在解码器中使用带有双线性上采样的1*1卷积融合不同的尺寸。并且将尺度为1/4预测进行上采样生成最终的结果。

核心代码：

```python
class BottleneckBlock(ResNetBlockBase):
    def __init__(
        self,
        in_channels,
        out_channels,
        *,
        bottleneck_channels,
        stride=1,
        num_groups=1,
        norm="BN",
        stride_in_1x1=False,
        dilation=1,
    ):
        if in_channels != out_channels:
            self.shortcut = Conv2d(
                in_channels,
                out_channels,
                kernel_size=1,
                stride=stride,
                bias=False,
                norm=get_norm(norm, out_channels),
            )
        else:
            self.shortcut = None

        stride_1x1, stride_3x3 = (stride, 1) if stride_in_1x1 else (1, stride)

        self.conv1 = Conv2d(
            in_channels,
            bottleneck_channels,
            kernel_size=1,
            stride=stride_1x1,
            bias=False,
            norm=get_norm(norm, bottleneck_channels),
        )

        self.conv2 = Conv2d(
            bottleneck_channels,
            bottleneck_channels,
            kernel_size=3,
            stride=stride_3x3,
            padding=1 * dilation,
            bias=False,
            groups=num_groups,
            dilation=dilation,
            norm=get_norm(norm, bottleneck_channels),
        )

        self.conv3 = Conv2d(
            bottleneck_channels,
            out_channels,
            kernel_size=1,
            bias=False,
            norm=get_norm(norm, out_channels),
        )

        for layer in [self.conv1, self.conv2, self.conv3, self.shortcut]:
            if layer is not None:  # shortcut can be None
                weight_init.c2_msra_fill(layer)

```





### 可分离卷积

论文中多次提到了使用的是可分离卷积，并不是传统的卷积。这里参考了<a href="https://www.cnblogs.com/wujianming-110117/p/12791986.html ">可分离卷积</a>

可分离卷积分为空间可分离卷积和深度可分离卷积。

#### 空间可分离卷积

空间可分离卷积主要处理图像和卷积核的空间维度:宽度和高度。空间可分离卷积简单地将卷积核划分为两个较小的卷积核。 最常见的情况是将3x3的卷积核划分为3x1和1x3的卷积 核，如下所示:

![img](D:\MarkDown\DeepLearning\img\1251718-20200428092426012-854144341.png)

这样就能将原来的9次乘积运算降低到6次，并达到了同样的效果，计算复杂性降低网络运算效率就得到了提到。

![img](D:\MarkDown\DeepLearning\img\1251718-20200428092509507-83952964.png)

#### 深度可分离卷积

与空间可分离卷积不同，深度可分离卷积与卷积核无法“分解”成两个较小的内核。 因此，它更常用。深度可分离卷积之所以如此命名，是因为它不仅涉及空间维度，还涉及深度维度（信道数量）。 输入图像可以具有3个信道：R、G、B。 在几次卷积之后，图像可以具有多个信道。

类似于空间可分离卷积，深度可分离卷积将卷积核分成两个单独的卷积核，这两个卷积核进行两个卷积：深度卷积和逐点卷积。 深度可分离卷积将执行一个空间卷积，同时保持通道独立，然后进行深度卷积操作。

假设有一个16输入通道和输出为32 ，卷积核为3x3的卷积层，那么就是I见16个通道中每一个都是由32个3x3的内核进行遍历，从而产生16*32=512个特征映射。通过将每个输入通道中的特征映射相加从而合成一个大的特征映射。由于我们可以进行此操作32次，因此我们得到了期望的32个输出通道。

如果使用深度可分离卷积，那么进行遍历16个通道，每个通道都有一个3x3的卷积核，可以输出为16个特征映射。与原来的计算次数为16x32x3x3 = 4068 使用深度可分离卷积后为16x3x3+16x32x1x1 = 656 参数大大下降。

